
================================================================================
QUANTUM DIAMOND FORGE - PROTOCOL v2.0 RED TEAM SNAPSHOT
Timestamp: 2025-11-23T16:24:22.443Z
Purpose: Independent analysis and review by external LLMs
================================================================================
Total Files: 46
--------------------------------------------------------------------------------

## Project Structure
```
GUIDE.md
INBOX/README.md
README.md
TASKS/done/001_harden-protocol-with-industry-benchmarks.md
diagrams/conceptual_overview(2.2).mermaid
diagrams/dependency_graph(2.2).mermaid
diagrams/high_level_flow(2.2).mermaid
diagrams/sequence_diagram(2.2).mermaid
docs/LOCKFILE_GUIDE.md
docs/VALIDATOR_GUIDE.md
guides/cicd/git_workflow.md
guides/cicd/github_setup.md
guides/cicd/how_to_commit.md
guides/cicd/overview.md
guides/cicd/security_scanning.md
prompts/00_MASTER_PROTOCOL.md
prompts/00_MASTER_PROTOCOL_LEAN.md
prompts/00_MASTER_PROTOCOL_ULTRA_LEAN.md
prompts/00_interactive_kickoff.md
prompts/01_requirements_analysis.md
prompts/02_architecture_design.md
prompts/03_security_compliance.md
prompts/04_testing_strategy.md
prompts/05_implementation_plan.md
prompts/REDTEAM/gpt5-response.md
prompts/REDTEAM/grok4-response.md
prompts/REDTEAM/redteam_analysis.md
prompts/REDTEAM/synthesis.md
prompts/lean/01_requirements_analysis.md
prompts/lean/02_architecture_design.md
prompts/lean/04_testing_strategy.md
prompts/lean/05_implementation_plan.md
prompts/lean/TEST_SCENARIO.md
prompts/modules/README.md
prompts/modules/architecture/c4_diagrams.md
prompts/modules/architecture/data_models.md
prompts/modules/requirements/feature_catalog.md
prompts/modules/requirements/product_overview.md
prompts/modules/requirements/user_stories.md
prompts/modules/security/threat_modeling.md
prompts/modules/testing/test_pyramid.md
prompts/ultra-lean/00_overview.md
prompts/ultra-lean/01_features.md
prompts/ultra-lean/02_system.md
prompts/ultra-lean/03_api.md
prompts/ultra-lean/04_plan.md
```

## File: GUIDE.md
```md
# The Quantum Diamond Forge Guide

**Status:** v20 (The "Universal Base" Edition)
**Philosophy:** "Don't prompt the ocean; build the aqueduct."

---

## 1. Introduction: Beyond "Vibe Coding"

We live in the age of **"Vibe Coding"**‚Äîwhere a single prompt can generate a stunning, functional app in seconds. It's magical. It captures the *idea*.

**But what happens next?**

Most AI-generated apps are "Prototype Plus"‚Äîfunctional but fragile. They look great but often lack the rigor required for the real world:
*   **Security Gaps:** Missing rate limiting, weak input validation, permissive CORS.
*   **Operational Fragility:** No structured logging, basic health checks, no observability.
*   **Maintenance Nightmares:** Ad-hoc patterns, no testing strategy, no architectural documentation.

### The Antigravity Solution
The **Quantum Diamond Forge** is the answer to this fragility. Built upon the principles of the [Quantum Diamond Framework](https://medium.com/@rfremmer_30873/beyond-the-double-diamond-design-process-a-new-framework-for-the-ai-age-the-quantum-diamond-980fe3f89319), this protocol operationalizes "Rigour at Speed."

It is not just a set of prompts; it is a **Universal Base** for orchestrating multiple AI agents to build software that is **Secure by Design**, **Operationally Mature**, and **Production Ready** from Day 1.

It addresses the common critiques of AI code (like those found in an [Antigravity Audit](#)) by enforcing:
*   **Strict Contracts:** Zod schemas for every input.
*   **Defense in Depth:** Helmet, Rate Limiting, and RBAC baked in.
*   **Observability:** Structured logging with Correlation IDs.
*   **Testing Pyramids:** Unit, Integration, and E2E tests as first-class citizens.

---

## 2. The Mental Model

1.  **The General (You + Local Agent):** You operate *locally*, inside this repository.
2.  **The Specialists (Web AIs):** ChatGPT (o1/4o) and Gemini (1.5 Pro) are your high-IQ consultants for "Precision Strikes".
3.  **The Expert Council:** A library of specialized personas (Architect, DevOps, Security) you can summon for deep-dive tasks.
4.  **The Shared Brain (The Repo):** This repository is the source of truth. It comes pre-loaded with **Governance** (ADRs), **Task Management**, and **Validation Scripts**.

---

## 3. Choose Your Mode

The Forge offers three tracks to match your project needs. Choose the one that fits your timeline and rigor requirements.

### ‚ö° Ultra-Lean Mode (Rapid Prototypes)
**Best for**: Weekend projects, hackathons, quick experiments
- **Time to Spec**: ~30 minutes
- **Artifacts**: 5 docs
- **Start here**: `prompts/00_MASTER_PROTOCOL_ULTRA_LEAN.md`

### üèÉ Lean Mode (Solo Devs & MVPs)
**Best for**: Prototypes, MVPs, solo developers, small teams
- **Time to Spec**: ~1 hour
- **Artifacts**: ~10 docs
- **Start here**: `prompts/00_MASTER_PROTOCOL_LEAN.md`

### üè¢ Enterprise Mode (Production Apps)
**Best for**: Production apps, teams, compliance-heavy projects
- **Time to Spec**: ~4-6 hours
- **Artifacts**: 25+ docs
- **Start here**: `prompts/00_MASTER_PROTOCOL.md`

---

## 4. The Workflow

### Conceptual Overview
This is how the entire system works from a developer's perspective.

```mermaid
flowchart TB
    Start([Developer with Idea]) --> Clone[Clone Quantum Diamond Forge]
    Clone --> IDE[Open in Antigravity IDE]
    
    %% --- Orchestration ---
    subgraph Orchestration ["üéØ Orchestration (Human-in-the-Loop)"]
        IDE --> Kickoff[Run ./forge.sh start]
        Kickoff --> Mode{Choose Mode<br/>(Ultra/Lean/Ent)}
        Mode --> Loop{Specification Loop}
    end
    
    %% --- Web LLM ---
    subgraph WebTools ["üß† Web LLM (The Heavy Lifter)"]
        Gemini["Web LLM e.g. Gemini 3 pro<br/>Generates Specs (per Mode)"]
    end
    
    %% --- Local Execution ---
    subgraph LocalExecution ["üíª Local Project (The Build)"]
        Artifacts["docs/ Folder<br/>1. Requirements<br/>2. Architecture<br/>3. Security<br/>4. Testing<br/>5. Implementation"]
        Agent["Antigravity Agent<br/>(The Builder)"]
        Code[Production Code]
    end
    
    %% Main Flow
    Loop -->|1. Copy Prompts| Gemini
    Gemini -->|2. Generate Docs<br/>(Detail Level = Mode)| Artifacts
    
    Artifacts -->|3. Guide| Agent
    Agent -->|4. Write| Code
    
    %% Feedback Loops
    Code -.->|Verify| Agent
    Agent -.->|Refine| Artifacts
    
    %% Final Output
    Code --> Done([Production-Ready Application])
    
    %% Styles
    style IDE fill:#dcfce7,stroke:#166534,stroke-width:3px
    style Done fill:#fef3c7,stroke:#d97706,stroke-width:3px
    style Artifacts fill:#dbeafe,stroke:#1e40af,stroke-width:2px
```

### High-Level Flow
This diagram shows the artifact generation pipeline and how the Expert Council integrates.

```mermaid
flowchart TB
    User["üë§ You (Orchestrator)"] -- "1. Kickoff<br/>(./forge.sh start -> Paste to Gemini)" --> Mode{"Choose Mode"}
    Mode -- "Ultra-Lean" --> Ultra["‚ö° 5 Artifacts"]
    Mode -- Lean --> Lean["üèÉ 10 Artifacts"]
    Mode -- Enterprise --> Ent["üè¢ 25+ Artifacts"]
    Ultra -- "2. Copy Prompts<br/>(./forge.sh prompt 1-5)" --> WebAI["üß† Web LLM<br>Gemini 3/GPT5/Claude4.5/Grok4/etc"]
    Lean --> WebAI
    Ent --> WebAI
    WebAI -- "3. Generates" --> Docs["üìÇ docs/ + üîê locks/<br>1. Requirements<br>2. Architecture<br>3. Security<br>4. Testing<br>5. Plan"]
    Docs -- "4. Validate" --> Gate{"‚úÖ Validator"}
    Gate -- Pass --> Agent["ü§ñ Antigravity Agent<br>(IDE)"]
    Gate -- Fail --> Docs
    Agent -- "5. Builds" --> Code["üíª Production Code"]
    Code -- Verify --> Tests{"‚úì Tests"}
    Tests -- Pass --> Done["‚úÖ Production Ready"]
    Tests -- Fail --> Agent

    style Docs fill:#dbeafe,stroke:#1e40af,stroke-width:2px
    style Gate fill:#fcd34d,stroke:#d97706,stroke-width:2px
    style Agent fill:#fef3c7,stroke:#d97706,stroke-width:2px
    style Done fill:#dcfce7,stroke:#166534,stroke-width:2px
```

### Detailed Sequence
This diagram shows the exact step-by-step interaction between you, the CLI, and the AIs.

```mermaid
sequenceDiagram
    autonumber
    actor User as üë§ You (Orchestrator)
    participant Agent as ü§ñ Antigravity IDE<BR> Agent
    participant CLI as üõ†Ô∏è Forge CLI
    participant WebAI as üß† Web LLM<BR>(Gemini/Grok/<BR>ChatGPT/Claude)
    participant Repo as üìÇ Local Repo<BR>(docs/)

    Note over User, Repo: Phase 1: The Specification Loop
    
    Note over User, Agent: Phase 0: Initialization
    
    User->>Agent: "I have an idea..."
    Agent-->>User: "Let's start. Run ./forge.sh start"
    
    User->>CLI: ./forge.sh start (Kickoff)
    CLI-->>User: Interactive Interview -> Mode Selection<br/>(Ultra-Lean / Lean / Enterprise)
    
    Note over User, Repo: Step 1: Requirements
    User->>CLI: ./forge.sh prompt 1 (Requirements)
    User->>WebAI: Paste Prompt + Idea
    WebAI-->>User: Generates Feature Catalog
    User->>Repo: Save docs/01_requirements_analysis.md
    User->>CLI: ./forge.sh lock requirements
    CLI-->>Repo: Generates requirements.lock.json
    
    Note over User, Repo: Step 2: Architecture
    User->>CLI: ./forge.sh prompt 2 (Architecture)
    User->>WebAI: Paste Prompt + Requirements Lock
    WebAI-->>User: Generates C4 Diagrams
    User->>Repo: Save docs/02_architecture_design.md
    User->>CLI: ./forge.sh lock architecture
    CLI-->>Repo: Generates architecture.lock.json
    
    Note over User, Repo: Step 3: Security
    User->>CLI: ./forge.sh prompt 3 (Security)
    User->>WebAI: Paste Prompt + Architecture Lock
    WebAI-->>User: Generates Threat Model
    User->>Repo: Save docs/03_security_compliance.md
    User->>CLI: ./forge.sh lock security
    CLI-->>Repo: Generates security.lock.json
    
    Note over User, Repo: Step 4: Testing
    User->>CLI: ./forge.sh prompt 4 (Testing)
    User->>WebAI: Paste Prompt + Context
    WebAI-->>User: Generates Test Strategy
    User->>Repo: Save docs/04_testing_strategy.md
    User->>CLI: ./forge.sh lock testing
    CLI-->>Repo: Generates testing.lock.json
    
    Note over User, Repo: Step 5: Plan
    User->>CLI: ./forge.sh prompt 5 (Plan)
    User->>WebAI: Paste Prompt + Context
    WebAI-->>User: Generates Implementation Plan
    User->>Repo: Save docs/05_implementation_plan.md
    User->>CLI: ./forge.sh lock implementation
    CLI-->>Repo: Generates implementation.lock.json
    
    Note over User, Agent: Phase 2: The Build Loop
    
    User->>CLI: ./forge.sh validate
    CLI-->>User: ‚úÖ All Checks Passed
    
    User->>Agent: Point to docs/locks/implementation.lock.json
    loop Antigravity Build Cycle
        Agent->>Repo: Read Lockfiles
        Agent->>Repo: Write Code
        Agent->>Repo: Run Tests
        Repo-->>Agent: Pass/Fail
    end
```

---

## 4. The Protocol Steps

### Step 0: Kickoff (The "Strategy")
**[üëâ View Guide](guides/00_kickoff.md)**
*   **Goal:** Initialize strategy and get a personalized roadmap.
*   **Action:** Run `./forge.sh start` -> Paste to Gemini 1.5 Pro.
    *   *Note:* This launches the **Interactive Antigravity Guide**, which will interview you about your idea and create a tailored plan.

### Step 1: Requirements Analysis (The "What")
**[üëâ View Guide](guides/01_requirements_analysis.md)**
*   **Goal:** Define the Narrative, Audience, and Core Features.
*   **Action:** Run `./forge.sh prompt 1` -> Paste to ChatGPT -> Save `docs/01_requirements_analysis.md`.

### Step 2: Architecture Design (The "How")
**[üëâ View Guide](guides/02_architecture_design.md)**
*   **Goal:** Define the Architecture, Stack, and Security.
*   **Action:** Run `./forge.sh prompt 2` -> Paste to ChatGPT (with Requirements) -> Save `docs/02_architecture_design.md`.

### Step 3: Security Compliance (The "Shield")
*   **Goal:** Define the Security Model and Compliance Requirements.
*   **Action:** Run `./forge.sh prompt 3` -> Paste to ChatGPT (with Architecture) -> Save `docs/03_security_compliance.md`.

### Step 4: Testing Strategy (The "Proof")
*   **Goal:** Define the Test Plan and Quality Gates.
*   **Action:** Run `./forge.sh prompt 4` -> Paste to ChatGPT (with Security) -> Save `docs/04_testing_strategy.md`.

### Step 5: Implementation Plan (The "Build")
*   **Goal:** Create the step-by-step build plan.
*   **Action:** Run `./forge.sh prompt 5` -> Paste to IDE Agent -> Save `docs/05_implementation_plan.md`.

---

## 5. Project Management Tools (Built-in)

The Forge comes with local tools to manage your project's governance.

### Tasks
Manage your backlog with structured markdown files.
```bash
./forge.sh task "Implement User Auth"
# Creates TASKS/backlog/001_implement_user_auth.md
```

### ADRs (Architectural Decision Records)
Document your technical decisions.
```bash
./forge.sh adr "Use Supabase for Auth"
# Creates docs/adr/0001-use-supabase-for-auth.md
```

### Code Snapshots
Capture your codebase context for the Expert Council.
```bash
# Capture everything (default)
./forge.sh snap output.txt

# Capture only code files (great for LLM context)
./forge.sh snap code_only.txt --mode code

# Capture specific folders (e.g., for a specific feature)
./forge.sh snap feature_context.txt --folders src/components,src/hooks
```

### Git Hooks (Automated Guardrails)
The Forge installs pre-configured Git hooks to keep your repo clean.
*   **Pre-commit:**
    *   Prevents commits to `main` branch.
    *   Blocks large files (>50MB).
    *   Runs `validate_drift.ts` (if present) to ensure code matches blueprint.
*   **Commit-msg:**
    *   Enforces [Conventional Commits](https://www.conventionalcommits.org/) (e.g., `feat: add login`).

### CI/CD & Workflow Guides
Detailed guides for setting up your pipeline and mastering the git workflow.
*   **[CI/CD Overview](guides/cicd/overview.md):** The big picture of how code moves from laptop to production.
*   **[GitHub Setup](guides/cicd/github_setup.md):** How to configure your repo for security and automation.
*   **[Git Workflow](guides/cicd/git_workflow.md):** Quick reference for branches, aliases, and commands.
*   **[How to Commit](guides/cicd/how_to_commit.md):** Step-by-step guide to passing the pre-commit hooks.
*   **[Security Scanning](guides/cicd/security_scanning.md):** Understanding and fixing vulnerability alerts.

### The Inbox Workflow
The `INBOX/` folder is your "Drop Zone" for AI collaboration.
1.  **Drop:** Drag messy notes, drafts, or external docs into `INBOX/`.
2.  **Prompt:** Tell the Agent "Check the Inbox."
3.  **Process:** The Agent will refactor, format, and move the files to their correct location (e.g., `docs/`, `TASKS/`).

---

## 6. The Expert Council (Specialized Roles)

Use these roles when you need deep expertise in a specific domain.

### How to Use
Run `./forge.sh role <role_name>` to copy the prompt.

*   **System Architect** (`architect`): Use for defining directory structures, ADRs, and technical constraints.
*   **Backend Developer** (`backend`): Use for implementing complex API logic, database schemas, and microservices.
*   **DevOps Engineer** (`devops`): Use for setting up CI/CD pipelines, Dockerfiles, and Infrastructure as Code.
*   **QA Engineer** (`qa`): Use for defining test strategies and writing E2E test suites.
*   **Security Auditor** (`security`): Use for threat modeling and security reviews.

---

## 7. Repository Deep Dive

*   `forge.sh`: The CLI tool.
*   `prompts/`: Core protocol prompts.
    *   `roles/`: The Expert Council prompts.
*   `guides/`: Detailed sub-guides.
*   `scripts/`: Automation tools (`new_task.sh`, `new_adr.sh`, `capture_snapshot.js`).
*   `templates/`: Source templates for artifacts.
*   `TASKS/`: Your project backlog.
*   `docs/adr/`: Your decision log.

```

## File: INBOX/README.md
```md
# Inbox

**Drop Zone for AI Processing**

Place any drafts, notes, or external documents here that you want the Antigravity Agent to review, refactor, or integrate into the project.

**Workflow:**
1. Drag files here.
2. Tell the Agent: "Check the Inbox."
3. The Agent will process them and move them to their permanent home (e.g., `docs/`, `TASKS/`).

```

## File: README.md
```md
# Quantum Diamond Forge üíé

**The Universal Base for AI-Native Development.**

[![CI](https://github.com/richfrem/quantum-diamond-forge/actions/workflows/ci.yml/badge.svg)](https://github.com/richfrem/quantum-diamond-forge/actions/workflows/ci.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> "Don't prompt the ocean; build the aqueduct."

The **Quantum Diamond Forge** is a protocol and scaffolding engine designed to operationalize "Rigour at Speed." It orchestrates AI agents (ChatGPT, Gemini, Claude) to build software that is **Secure by Design**, **Operationally Mature**, and **Production Ready** from Day 1.

## üöÄ Why?

Most AI-generated code is "Prototype Plus"‚Äîfunctional but fragile. It lacks:
*   **Security:** Rate limiting, input validation, headers.
*   **Ops:** Logging, health checks, Dockerfiles.
*   **Structure:** Consistent architecture and testing patterns.

The Forge solves this by providing a **Universal Base** of prompts, templates, and validation scripts that enforce quality gates at every step.

## üõ†Ô∏è Features

*   **Protocol-Driven Workflow:** A 5-step granular process (Requirements ‚Üí Architecture ‚Üí Security ‚Üí Testing ‚Üí Implementation).
*   **Expert Council:** Specialized AI personas (Architect, DevOps, Security) for deep-dive tasks.
*   **Automated Governance:** Built-in ADRs, Task Management, and Git Hooks.
*   **Defense in Depth:** Pre-configured security headers, Zod validation, and drift detection.

## ‚ö° Quick Start: Protocol v2.1

The Forge offers **three tracks** to match your project needs:

### ‚ö° Ultra-Lean Mode (Rapid Prototypes)
**Best for**: Weekend projects, hackathons, quick experiments

- **Time to Spec**: ~30 minutes
- **Artifacts**: 5 docs
- **Focus**: Ship it this weekend
- **Start here**: `prompts/00_MASTER_PROTOCOL_ULTRA_LEAN.md`

### üèÉ Lean Mode (Solo Devs & MVPs)
**Best for**: Prototypes, MVPs, solo developers, small teams

- **Time to Spec**: ~1 hour
- **Artifacts**: ~10 docs
- **Focus**: Essential features only
- **Start here**: `prompts/00_MASTER_PROTOCOL_LEAN.md`

### üè¢ Enterprise Mode (Production Apps)
**Best for**: Production apps, teams, compliance-heavy projects

- **Time to Spec**: ~4-6 hours
- **Artifacts**: 25+ docs
- **Focus**: Full rigor (C4 Level 1-3, STRIDE, compliance)
- **Start here**: `prompts/00_MASTER_PROTOCOL.md`

---

### The Workflow (All Modes)
```mermaid
flowchart TB
    User["üë§ You (Orchestrator)"] -- "1. Kickoff<br/>(./forge.sh start -> Paste to Gemini)" --> Mode{"Choose Mode"}
    Mode -- "Ultra-Lean" --> Ultra["‚ö° 5 Artifacts"]
    Mode -- Lean --> Lean["üèÉ 10 Artifacts"]
    Mode -- Enterprise --> Ent["üè¢ 25+ Artifacts"]
    Ultra -- "2. Copy Prompts<br/>(./forge.sh prompt 1-5)" --> WebAI["üß† Web LLM<br>Gemini 3/GPT5/Claude4.5/Grok4/etc"]
    Lean --> WebAI
    Ent --> WebAI
    WebAI -- "3. Generates" --> Docs["üìÇ docs/ + üîê locks/<br>1. Requirements<br>2. Architecture<br>3. Security<br>4. Testing<br>5. Plan"]
    Docs -- "4. Validate" --> Gate{"‚úÖ Validator"}
    Gate -- Pass --> Agent["ü§ñ Antigravity Agent<br>(IDE)"]
    Gate -- Fail --> Docs
    Agent -- "5. Builds" --> Code["üíª Production Code"]
    Code -- Verify --> Tests{"‚úì Tests"}
    Tests -- Pass --> Done["‚úÖ Production Ready"]
    Tests -- Fail --> Agent

    style Docs fill:#dbeafe,stroke:#1e40af,stroke-width:2px
    style Gate fill:#fcd34d,stroke:#d97706,stroke-width:2px
    style Agent fill:#fef3c7,stroke:#d97706,stroke-width:2px
    style Done fill:#dcfce7,stroke:#166534,stroke-width:2px
```

### Step-by-Step

1.  **Clone the Forge:**
    ```bash
    git clone https://github.com/richfrem/quantum-diamond-forge.git
    cd quantum-diamond-forge
    ```

2.  **Start the Interactive Kickoff:**
    ```bash
    ./forge.sh start
    ```
    This will interview you and guide you to the right mode (Ultra-Lean, Lean, or Enterprise).

3.  **Phase 1: The Specification Loop**
    Follow the prompts to generate your documentation suite.
    *   **Lock It Down**: Run `./forge.sh lock all` to prevent drift.
    *   **Validate**: Run `./forge.sh validate` to check consistency.

4.  **Phase 2: The Build Loop**
    *   **Build**: Run `./forge.sh build` to instruct the Agent.
    *   **Test**: Run `./forge.sh test` to verify the build.

## üìö Documentation

*   **[The Guide (Start Here)](GUIDE.md)**: The complete manual for the protocol.
*   **[CI/CD & Workflow](guides/cicd/overview.md)**: How to ship code securely.
*   **[Expert Roles](prompts/roles/)**: Browse the specialized personas.

## ü§ù Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for details.

## üìÑ License

This project is licensed under the [MIT License](LICENSE).

```

## File: TASKS/done/001_harden-protocol-with-industry-benchmarks.md
```md
# Task: Harden Protocol with Industry Benchmarks

**Status**: Done
**Created**: 2025-11-22
**Owner**: Antigravity

### Phase 2: Prompt Engineering
- [x] Create `prompts/01_requirements_analysis.md`
- [x] Create `prompts/02_architecture_design.md`
- [x] Create `prompts/03_security_compliance.md`
- [x] Create `prompts/04_testing_strategy.md`
- [x] Create `prompts/05_implementation_plan.md`
- [x] Remove legacy prompts (`01_product_spec.md`, etc.)
- [x] Create `prompts/00_MASTER_PROTOCOL.md` (The Guide)
- [x] Remove legacy kickoff files (`00_kickoff.md`, `00_interactive_kickoff.md`)
- [x] Update `README.md` with Protocol v2.0 workflow & Mermaid diagram
- [x] Update `README.md` with Protocol v2.0 workflow & Mermaid diagram
- [x] Refine `prompts/00_MASTER_PROTOCOL.md` to clarify Orchestrator role
- [x] Update `diagrams/workflow_map.mermaid`
- [x] Update `diagrams/conceptual_overview.mermaid`
- [x] Update `diagrams/sequence_diagram.mermaid`
- [x] Update `diagrams/dependency_graph.mermaid`

## Context
We analyzed enterprise-grade technical specifications (e.g., the "Helix" spec) to benchmark the Quantum Diamond Forge (QDF) protocol. The goal is to elevate QDF from generating "MVP-style" outputs to "Enterprise-Grade" specifications.

## Benchmark Analysis & Gap Report
The industry benchmark specs demonstrated superior rigor in several areas where QDF v1 was lacking:

| Area | Industry Benchmark (Helix Spec) | QDF v1 (Old Prompts) | Gap |
|------|---------------------------------|----------------------|-----|
| **Requirements** | Detailed Feature Catalog, MoSCoW, Functional/Non-Functional Requirements, Traceability Matrix. | Simple "Narrative" and "Core Feature Scope". | **High**: QDF lacked structured requirement definitions and traceability. |
| **Architecture** | C4 Diagrams, Event Flows, Detailed Component interactions. | High-level stack list and basic "Domain Architecture". | **Medium**: QDF needed standard diagramming (C4) and deeper API design. |
| **Security** | Explicit Threat Modeling, Compliance Controls (DSCSA, GDPR), AuthZ matrix. | "Shift Left" bullet points, generic security headers. | **Critical**: QDF lacked specific threat modeling and compliance mapping. |
| **Testing** | Test Pyramid, Performance criteria, Security testing (SAST/DAST). | Basic "CI/CD Pipeline" steps. | **High**: QDF missed a dedicated testing strategy. |

## Objective
Upgrade QDF to **Protocol v2.0** by implementing a rigorous 5-step prompting strategy that mimics enterprise depth.

## Implementation Checklist

### Phase 1: Protocol Design (v2.0)
- [x] **Analyze Benchmarks**: Extract patterns for Requirements, Architecture, and Security.
- [x] **Design New Prompt Architecture**:
    - `01_requirements_analysis.md`: Feature Catalog, User Stories.
    - `02_architecture_design.md`: C4, API, Data Models.
    - `03_security_compliance.md`: Threat Modeling, Compliance.
    - `04_testing_strategy.md`: Test Pyramid, Performance.
    - `05_implementation_plan.md`: Task Breakdown.

```

## File: diagrams/conceptual_overview(2.2).mermaid
```mermaid
flowchart TB
    Start([Developer with Idea]) --> Clone[Clone Quantum Diamond Forge]
    Clone --> IDE[Open in Antigravity IDE]
    
    %% --- Orchestration ---
    subgraph Orchestration ["üéØ Orchestration (Human-in-the-Loop)"]
        IDE --> Kickoff[Run ./forge.sh start]
        Kickoff --> Mode{Choose Mode<br/>(Ultra/Lean/Ent)}
        Mode --> Loop{Specification Loop}
    end
    
    %% --- Web LLM ---
    subgraph WebTools ["üß† Web LLM (The Heavy Lifter)"]
        Gemini["Web LLM e.g. Gemini 3 pro<br/>Generates Specs (per Mode)"]
    end
    
    %% --- Local Execution ---
    subgraph LocalExecution ["üíª Local Project (The Build)"]
        Artifacts["docs/ Folder<br/>1. Requirements<br/>2. Architecture<br/>3. Security<br/>4. Testing<br/>5. Implementation"]
        Agent["Antigravity Agent<br/>(The Builder)"]
        Code[Production Code]
    end
    
    %% Main Flow
    Loop -->|1. Copy Prompts| Gemini
    Gemini -->|2. Generate Docs<br/>(Detail Level = Mode)| Artifacts
    
    Artifacts -->|3. Guide| Agent
    Agent -->|4. Write| Code
    
    %% Feedback Loops
    Code -.->|Verify| Agent
    Agent -.->|Refine| Artifacts
    
    %% Final Output
    Code --> Done([Production-Ready Application])
    
    %% Styles
    style IDE fill:#dcfce7,stroke:#166534,stroke-width:3px
    style Done fill:#fef3c7,stroke:#d97706,stroke-width:3px
    style Artifacts fill:#dbeafe,stroke:#1e40af,stroke-width:2px

```

## File: diagrams/dependency_graph(2.2).mermaid
```mermaid
---
config:
  layout: elk
---
flowchart TB
    Req["01_requirements_analysis.md<br>(Feature Catalog)"] --> Arch["02_architecture_design.md<br>(C4 &amp; Data Models)"] & Sec["03_security_compliance.md<br>(Threat Models)"] & Test["04_testing_strategy.md<br>(Test Strategy)"] & Plan["05_implementation_plan.md<br>(Task Breakdown)"]
    Arch --> Sec & Test & Plan
    Sec --> Plan
    Test --> Plan

    style Req fill:#dbeafe,stroke:#1e40af,stroke-width:2px
    style Plan fill:#dcfce7,stroke:#166534,stroke-width:2px
```

## File: diagrams/high_level_flow(2.2).mermaid
```mermaid
flowchart TB
    User["üë§ You (Orchestrator)"] -- "1. Kickoff<br/>(./forge.sh start -> Paste to Gemini)" --> Mode{"Choose Mode"}
    Mode -- "Ultra-Lean" --> Ultra["‚ö° 5 Artifacts"]
    Mode -- Lean --> Lean["üèÉ 10 Artifacts"]
    Mode -- Enterprise --> Ent["üè¢ 25+ Artifacts"]
    Ultra -- "2. Copy Prompts<br/>(./forge.sh prompt 1-5)" --> WebAI["üß† Web LLM<br>Gemini 3/GPT5/Claude4.5/Grok4/etc"]
    Lean --> WebAI
    Ent --> WebAI
    WebAI -- "3. Generates" --> Docs["üìÇ docs/ + üîê locks/<br>1. Requirements<br>2. Architecture<br>3. Security<br>4. Testing<br>5. Plan"]
    Docs -- "4. Validate" --> Gate{"‚úÖ Validator"}
    Gate -- Pass --> Agent["ü§ñ Antigravity Agent<br>(IDE)"]
    Gate -- Fail --> Docs
    Agent -- "5. Builds" --> Code["üíª Production Code"]
    Code -- Verify --> Tests{"‚úì Tests"}
    Tests -- Pass --> Done["‚úÖ Production Ready"]
    Tests -- Fail --> Agent

    style Docs fill:#dbeafe,stroke:#1e40af,stroke-width:2px
    style Gate fill:#fcd34d,stroke:#d97706,stroke-width:2px
    style Agent fill:#fef3c7,stroke:#d97706,stroke-width:2px
    style Done fill:#dcfce7,stroke:#166534,stroke-width:2px
```

## File: diagrams/sequence_diagram(2.2).mermaid
```mermaid
sequenceDiagram
    autonumber
    actor User as üë§ You (Orchestrator)
    participant Agent as ü§ñ Antigravity IDE<BR> Agent
    participant CLI as üõ†Ô∏è Forge CLI
    participant WebAI as üß† Web LLM<BR>(Gemini/Grok/<BR>ChatGPT/Claude)
    participant Repo as üìÇ Local Repo<BR>(docs/)

    Note over User, Repo: Phase 1: The Specification Loop
    
    Note over User, Agent: Phase 0: Initialization
    
    User->>Agent: "I have an idea..."
    Agent-->>User: "Let's start. Run ./forge.sh start"
    
    User->>CLI: ./forge.sh start (Kickoff)
    CLI-->>User: Interactive Interview -> Mode Selection<br/>(Ultra-Lean / Lean / Enterprise)
    
    Note over User, Repo: Step 1: Requirements
    User->>CLI: ./forge.sh prompt 1 (Requirements)
    User->>WebAI: Paste Prompt + Idea
    WebAI-->>User: Generates Feature Catalog
    User->>Repo: Save docs/01_REQUIREMENTS.md
    User->>CLI: ./forge.sh lock requirements
    CLI-->>Repo: Generates requirements.lock.json
    
    Note over User, Repo: Step 2: Architecture
    User->>CLI: ./forge.sh prompt 2 (Architecture)
    User->>WebAI: Paste Prompt + Requirements Lock
    WebAI-->>User: Generates C4 Diagrams
    User->>Repo: Save docs/02_ARCHITECTURE.md
    User->>CLI: ./forge.sh lock architecture
    CLI-->>Repo: Generates architecture.lock.json
    
    Note over User, Repo: Step 3: Security
    User->>CLI: ./forge.sh prompt 3 (Security)
    User->>WebAI: Paste Prompt + Architecture Lock
    WebAI-->>User: Generates Threat Model
    User->>Repo: Save docs/03_SECURITY.md
    User->>CLI: ./forge.sh lock security
    CLI-->>Repo: Generates security.lock.json
    
    Note over User, Repo: Step 4: Testing
    User->>CLI: ./forge.sh prompt 4 (Testing)
    User->>WebAI: Paste Prompt + Context
    WebAI-->>User: Generates Test Strategy
    User->>Repo: Save docs/04_TESTING.md
    User->>CLI: ./forge.sh lock testing
    CLI-->>Repo: Generates testing.lock.json
    
    Note over User, Repo: Step 5: Plan
    User->>CLI: ./forge.sh prompt 5 (Plan)
    User->>WebAI: Paste Prompt + Context
    WebAI-->>User: Generates Implementation Plan
    User->>Repo: Save docs/05_IMPLEMENTATION.md
    User->>CLI: ./forge.sh lock implementation
    CLI-->>Repo: Generates implementation.lock.json
    
    Note over User, Agent: Phase 2: The Build Loop
    
    User->>CLI: ./forge.sh validate
    CLI-->>User: ‚úÖ All Checks Passed
    
    User->>Agent: Point to docs/locks/implementation.lock.json
    loop Antigravity Build Cycle
        Agent->>Repo: Read Lockfiles
        Agent->>Repo: Write Code
        Agent->>Repo: Run Tests
        Repo-->>Agent: Pass/Fail
    end

```

## File: docs/LOCKFILE_GUIDE.md
```md
# Lockfile System Guide

## Overview
Lockfiles prevent spec drift by creating versioned, hashed snapshots of your artifacts. The IDE agent reads lockfiles instead of raw markdown, ensuring consistency.

## Why Lockfiles?

**Problem**: As projects evolve, artifacts diverge:
- Requirements change, but architecture doesn't update
- API spec contradicts data models
- Security model references non-existent endpoints

**Solution**: Lockfiles act as a "single source of truth" that:
- Track changes via hashing
- Enforce dependencies between artifacts
- Enable validation before building

## Lockfile Structure

```json
{
  "version": "1.0.0",
  "generated": "2025-11-22T19:00:00Z",
  "source": "docs/01_REQUIREMENTS.md",
  "hash": "sha256:abc123...",
  "dependencies": [],
  "content": {
    "features": ["Feature 1", "Feature 2"],
    "userStories": ["US-001: ...", "US-002: ..."],
    "nfrs": {}
  }
}
```

## Usage

### Generate Lockfiles

```bash
# Generate all lockfiles
node scripts/generate_lockfile.js all

# Generate single lockfile
node scripts/generate_lockfile.js requirements
node scripts/generate_lockfile.js architecture
```

### Validate Lockfiles

```bash
# Validate all lockfiles
node scripts/validate_lockfiles.js
```

**Checks performed**:
- ‚úÖ Hash integrity (detect if source changed)
- ‚úÖ API ‚Üî Data Model consistency
- ‚úÖ Requirements ‚Üî Tests coverage
- ‚úÖ Security ‚Üî Endpoints mapping (future)

### Workflow Integration

**After generating specs**:
1. Generate lockfiles: `node scripts/generate_lockfile.js all`
2. Validate: `node scripts/validate_lockfiles.js`
3. If validation passes, proceed to IDE agent
4. IDE agent reads lockfiles from `docs/locks/`

**When specs change**:
1. Update markdown artifact
2. Regenerate lockfile: `node scripts/generate_lockfile.js <artifact>`
3. Validate again
4. Commit both markdown and lockfile

## Lockfile Locations

```
docs/locks/
‚îú‚îÄ‚îÄ requirements.lock.json
‚îú‚îÄ‚îÄ architecture.lock.json
‚îú‚îÄ‚îÄ security.lock.json
‚îú‚îÄ‚îÄ testing.lock.json
‚îî‚îÄ‚îÄ implementation.lock.json
```

## Dependencies

Lockfiles track dependencies:
- `requirements` ‚Üí no dependencies
- `architecture` ‚Üí depends on `requirements`
- `security` ‚Üí depends on `requirements`, `architecture`
- `testing` ‚Üí depends on `requirements`, `architecture`
- `implementation` ‚Üí depends on all above

## Example Validation Output

```
üîç Quantum Diamond Forge - Lockfile Validator

üìã Checking hash integrity...
‚úÖ requirements: OK
‚úÖ architecture: OK
‚ùå security: Hash mismatch - source file changed

üìã Checking API ‚Üî Data Model consistency...
‚ö†Ô∏è  Warnings:
   - API endpoint POST /api/users may not have corresponding data model
   - Data model 'Task' is defined but not used in any API

üìã Checking Requirements ‚Üî Tests coverage...
‚úÖ Requirements ‚Üî Tests: OK

üìä Summary:
   Errors: 1
   Warnings: 2

‚ùå Validation failed. Fix errors before proceeding.
```

## Best Practices

1. **Generate lockfiles after every spec change**
2. **Commit lockfiles with markdown** (both should be in sync)
3. **Run validation before starting implementation**
4. **Fix errors immediately** (don't ignore validation failures)
5. **Review warnings** (they may indicate real issues)

## Future Enhancements

- Auto-generate lockfiles on save (watch mode)
- Git pre-commit hook integration
- Diff tool to compare lockfile versions
- Auto-fix capabilities for common issues

```

## File: docs/VALIDATOR_GUIDE.md
```md
# Spec Validator CLI Guide

## Overview
The **Spec Validator** (`./forge.sh validate`) is your automated quality assurance tool for the specification phase. It ensures that your artifacts (Requirements, Architecture, API, etc.) remain consistent, complete, and drift-free.

## Why Use It?
As your project grows, keeping 5+ markdown files in sync is hard. The validator automates this by checking:
- **Consistency**: Does the API match the Data Model?
- **Completeness**: Do all Requirements have Tests?
- **Integrity**: Have the source files changed without updating the lockfiles?

## Usage

### Basic Validation
Run this command to check all artifacts:
```bash
./forge.sh validate
```

**Example Output:**
```
üîç Quantum Diamond Forge - Spec Validation Report

‚úÖ Hash Integrity Check
‚ö†Ô∏è  API ‚Üî Data Model Consistency
   üî∏ Endpoint POST /api/users references undefined model 'UserProfile'
‚úÖ Requirements ‚Üî Tests Coverage

üìä Summary:
   Errors: 0
   Warnings: 1
```

### Auto-Fixing Issues
The validator can automatically fix common issues, such as missing lockfiles.
```bash
./forge.sh validate --fix
```

### JSON Output (CI/CD)
Generate a machine-readable report for CI pipelines.
```bash
./forge.sh validate --report json
```

## Validation Rules

### 1. Hash Integrity Check
**What it checks**: Ensures that the markdown source files (`docs/*.md`) haven't changed since the last lockfile generation.
**Why**: Prevents "Spec Drift" where the IDE agent works off outdated specs.
**Fix**: Run `./forge.sh lock all` or use `./forge.sh validate --fix`.

### 2. API ‚Üî Data Model Consistency
**What it checks**:
- **Missing Models**: API endpoints referencing data models that don't exist.
- **Unused Models**: Data models defined but never used in any API.
**Why**: Ensures your architecture is coherent.

### 3. Requirements ‚Üî Tests Coverage
**What it checks**: Ensures that if you have defined features in `01_REQUIREMENTS.md`, you also have a testing strategy in `04_TESTING.md`.
**Why**: Prevents "Untested Features".

## Integration with Workflow

1. **Edit Specs**: You update `docs/01_REQUIREMENTS.md`.
2. **Validate**: Run `./forge.sh validate`.
   - It will fail the **Hash Integrity Check**.
3. **Fix**: Run `./forge.sh validate --fix` (or `./forge.sh lock requirements`).
   - This updates the lockfile.
4. **Re-Validate**: Run `./forge.sh validate`.
   - Now it checks for logical inconsistencies.
5. **Commit**: `git commit -am "Update specs"`

## Troubleshooting

**"Hash mismatch for..."**
- You edited a markdown file but didn't update the lockfile.
- **Fix**: Run `./forge.sh lock <artifact>` or `./forge.sh validate --fix`.

**"Endpoint ... references undefined model..."**
- Your API definition uses a model name that isn't in your Data Models section.
- **Fix**: Add the model to `docs/02_ARCHITECTURE.md` or correct the API definition.

**"Found X features but no testing lockfile..."**
- You defined requirements but haven't created `docs/04_TESTING.md` yet.
- **Fix**: Create the testing doc or run the Testing Strategy prompt.

```

## File: guides/cicd/git_workflow.md
```md
# Git Workflow Quick Reference

This guide provides recommended git workflows and shortcuts for the quantum-diamond-forge project.

## TL;DR - Recommended Setup

```bash
# 1. Add these aliases to your ~/.gitconfig
git config --global alias.st "status -sb"
git config --global alias.aa "add --all"
git config --global alias.cm "commit -m"
git config --global alias.lg "log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit"

# 2. Use conventional commits (see examples below)

# 3. Let pre-commit hooks validate your changes
```

## Conventional Commit Format

```
<type>(<scope>): <subject>

<body>

<footer>
```

### Commit Types

| Type | When to Use | Example |
|------|-------------|---------|
| `feat` | New feature | `feat(auth): add OAuth2 login` |
| `fix` | Bug fix | `fix(api): handle null user response` |
| `docs` | Documentation only | `docs(readme): update setup instructions` |
| `style` | Code formatting (no logic change) | `style(components): fix indentation` |
| `refactor` | Code restructuring | `refactor(utils): extract validation logic` |
| `test` | Adding/updating tests | `test(api): add integration tests` |
| `chore` | Maintenance tasks | `chore(deps): update dependencies` |
| `ci` | CI/CD changes | `ci(github): add CodeQL workflow` |
| `perf` | Performance improvements | `perf(db): optimize query performance` |
| `revert` | Revert previous commit | `revert: revert feat(auth) commit` |

## Common Workflows

### 1. Feature Development (Standard)

```bash
# Create feature branch
git checkout -b feature/add-security-scanning

# Make changes, then stage specific files
git add .github/dependabot.yml
git add .github/workflows/codeql.yml
git add docs/ci-cd/README.md

# Review what you're about to commit
git diff --cached

# Commit with conventional format
git commit -m "feat(security): configure GitHub Advanced Security

- Add Dependabot for dependency scanning
- Add CodeQL for security analysis
- Update CI/CD docs with security guide

Refs: TASK-0067, ADR-040"

# Push to remote
git push origin feature/add-security-scanning

# Create PR on GitHub
# After PR approval, merge via GitHub UI
```

### 2. Quick Fix (Using Aliases)

```bash
# Fix a typo in documentation
git aa  # Stage all changes
git cm "docs(readme): fix typo in installation steps"
git push
```

### 3. Multi-file Changes (Interactive Staging)

```bash
# Stage specific lines from files
git add -p

# Review staged changes
git diff --cached

# Commit
git commit -m "refactor(api): extract error handling logic"

# Push
git push
```

### 4. Amend Last Commit

```bash
# Forgot to add a file to last commit
git add forgotten-file.js
git commit --amend --no-edit

# Or change the commit message
git commit --amend -m "feat(auth): add OAuth2 login (updated message)"

# Force push (only if not yet merged!)
git push --force-with-lease
```

## Useful Git Aliases

Add these to your `~/.gitconfig`:

```gitconfig
[alias]
    # Quick status
    st = status -sb

    # Stage all changes
    aa = add --all

    # Commit with message
    cm = commit -m

    # Amend last commit
    amend = commit --amend --no-edit

    # Pretty log
    lg = log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit

    # Show staged changes
    staged = diff --cached

    # Undo last commit (keep changes)
    undo = reset HEAD~1

    # List branches sorted by last modified
    branches = branch --sort=-committerdate

    # Show files in last commit
    last = show --name-only
```

## Pre-commit Hook Usage

Our pre-commit hook (`.githooks/pre-commit`) automatically validates:
- ‚úÖ No `.env` files (except `.env.example`)
- ‚úÖ No hardcoded secrets (API keys, tokens, passwords)
- ‚úÖ Blocks commit if violations found

**If the hook blocks your commit:**

```bash
# Review the violation
# Fix the issue (remove secret, use environment variable)
# Try committing again

# ONLY bypass if you're absolutely sure it's safe
git commit --no-verify -m "your message"
```

**‚ö†Ô∏è WARNING:** Never use `--no-verify` to bypass secret detection!

## Branch Naming Conventions

```
feature/short-description    # New features
fix/bug-description          # Bug fixes
docs/documentation-update    # Documentation
refactor/code-improvement    # Code refactoring
test/add-tests              # Test additions
chore/maintenance-task      # Maintenance
```

**Examples:**
- `feature/github-security-scanning`
- `fix/null-pointer-in-auth`
- `docs/update-ci-cd-guide`
- `refactor/extract-validation-logic`

## Commit Message Examples

### Good Commit Messages ‚úÖ

```bash
# Feature with detailed body
git commit -m "feat(security): add Dependabot and CodeQL workflows

- Configure Dependabot for npm and GitHub Actions
- Add CodeQL workflow for JavaScript/TypeScript analysis
- Update CI/CD documentation with security scanning guide

This implements the security scanning layer documented in ADR-040.

Refs: TASK-0067, ADR-040"

# Bug fix with issue reference
git commit -m "fix(auth): handle null user response from Supabase

Fixes #123"

# Documentation update
git commit -m "docs(ci-cd): add security scanning interpretation guide"

# Dependency update
git commit -m "chore(deps): bump axios from 0.21.1 to 1.6.0

Fixes CVE-2023-45857 (High severity)"
```

### Bad Commit Messages ‚ùå

```bash
# Too vague
git commit -m "fix stuff"
git commit -m "update files"
git commit -m "changes"

# No type prefix
git commit -m "added security scanning"

# Too long subject line (>72 chars)
git commit -m "feat(security): add Dependabot and CodeQL workflows for automated dependency scanning and security analysis"
```

## IDE Git Integration

### Visual Studio Code

1. **Stage files:** Click `+` next to file in Source Control panel
2. **Review changes:** Click file to see diff
3. **Commit:** Type message in input box, press `Ctrl+Enter`
4. **Push:** Click `...` ‚Üí Push

**Recommended extensions:**
- GitLens - Enhanced git capabilities
- Git Graph - Visualize branch history

### JetBrains IDEs (WebStorm, IntelliJ)

1. **Commit:** `Ctrl+K` (Windows/Linux) or `Cmd+K` (Mac)
2. **Review changes:** Check boxes for files to stage
3. **Commit message:** Type in message box
4. **Commit and Push:** Click dropdown ‚Üí Commit and Push

## Troubleshooting

### Pre-commit hook not running

```bash
# Make hook executable
chmod +x .githooks/pre-commit

# Verify git hooks path
git config core.hooksPath .githooks
```

### Accidentally committed secret

```bash
# 1. IMMEDIATELY revoke the secret in the service provider
# 2. Remove from git history
git filter-branch --force --index-filter \
  "git rm --cached --ignore-unmatch path/to/file" \
  --prune-empty --tag-name-filter cat -- --all

# 3. Force push (‚ö†Ô∏è coordinate with team!)
git push origin --force --all

# 4. Update environment variables with new secret
```

### Merge conflict

```bash
# 1. Pull latest changes
git pull origin main

# 2. Resolve conflicts in your editor
# Look for <<<<<<< HEAD markers

# 3. Stage resolved files
git add resolved-file.js

# 4. Complete merge
git commit -m "merge: resolve conflicts with main"

# 5. Push
git push
```

## Best Practices

1. **Commit often** - Small, focused commits are easier to review and revert
2. **Write clear messages** - Future you will thank present you
3. **Review before committing** - Always run `git diff --cached`
4. **Test before pushing** - Run `npm run lint` and `npm run test:unit`
5. **Pull before pushing** - Avoid merge conflicts
6. **Use branches** - Never commit directly to `main`
7. **Keep commits atomic** - One logical change per commit

## References

- [Conventional Commits](https://www.conventionalcommits.org/)
- [Git Documentation](https://git-scm.com/doc)
- [GitHub Flow](https://docs.github.com/en/get-started/quickstart/github-flow)
- ADR-041: Git Workflow Automation Strategy

```

## File: guides/cicd/github_setup.md
```md
# GitHub Repository Configuration Guide

This guide documents how to configure the **Quantum Diamond Forge** GitHub repository to enable CI/CD pipelines, security scanning, and automated workflows.

## Prerequisites

- Admin access to the GitHub repository
- Repository: `https://github.com/richfrem/quantum-diamond-forge`

## Best Practices for AI-Assisted Development

**‚ö†Ô∏è Important for Antigravity IDE Agents:**

When working with CI/CD pipelines that include security scans (CodeQL, Dependabot), follow these practices to minimize overhead:

1. **Batch commits locally** - Make multiple commits on your feature branch before pushing
2. **Push once when ready** - Only push when the feature is complete and tested locally
3. **Use draft PRs** - Mark PRs as "Draft" while still working, convert to "Ready for review" when done
4. **Avoid rapid push cycles** - CI scans (especially CodeQL) can take 1-2 minutes per run

**Why:** Security scans are resource-intensive. Pushing every small change creates unnecessary CI runs and slows down the development workflow.

**Recommended workflow:**
```bash
# Make multiple commits locally
git commit -m "feat: add feature part 1"
git commit -m "feat: add feature part 2"
git commit -m "fix: address edge case"

# Push once when ready
git push origin feature/my-feature

# Create PR (mark as draft if still WIP)
gh pr create --draft --title "WIP: My Feature"
```

## Step 1: Enable GitHub Actions (done)

GitHub Actions should be enabled by default, but verify:

1. Go to **Settings** ‚Üí **Actions** ‚Üí **General**
2. Under "Actions permissions", select:
   - ‚úÖ **Allow all actions and reusable workflows**
3. Under "Workflow permissions", select:
   - ‚úÖ **Read and write permissions**
   - ‚úÖ **Allow GitHub Actions to create and approve pull requests**
4. Click **Save**

## Step 2: Enable Security Features

1. Go to **Settings** ‚Üí **Code security and analysis** (Sidebar under "Security").
2. Under the **Advanced Security** section, **Enable** the following:
   - **Dependency graph** (Should be enabled by default)
   - **Dependabot alerts**
   - **Dependabot security updates**
     - *Optional:* Enable **Grouped security updates** to reduce noise.
   - **Secret Protection** -> **Push protection** (Block commits that contain supported secrets).
   - **Private vulnerability reporting** (Optional).

## Step 3: Configure CodeQL Analysis

**Eligibility:**
- **Public repositories:** Free for everyone.
- **Private repositories:** Requires GitHub Advanced Security (GHAS) license.

**Setup Instructions:**
1. Still in **Code security and analysis**, scroll down to **Code scanning** / **CodeQL analysis**.
2. Click **Set up** (or "Configure").
3. Choose **Default** setup (Recommended).
   - GitHub will automatically detect languages (JavaScript/TypeScript, Python, etc.).
   - It will create a dynamic workflow without you needing to commit a YAML file.
   - Click **Enable CodeQL**.

*(If "Default" is not available, choose "Advanced" and it will generate a `codeql.yml` file for you to commit).*

## Step 4: Create Development Branch

Before setting up branch protection, create a `dev` branch for integration testing:

```bash
# Make sure you're on main and up to date
git checkout main
git pull origin main

# Create dev branch from main
git checkout -b dev
git push -u origin dev

# Return to your working branch
git checkout -
```

## Step 5: Configure Branch Protection Rules

### 5.1 Protect the `main` Branch

1. Go to **Settings** ‚Üí **Branches**
2. Click **Add branch protection rule**
3. **Branch name pattern:** `main`
4. Enable:
   - ‚úÖ **Require a pull request before merging**
   - ‚ùå **Require approvals** - UNCHECK (not needed for solo dev, check for teams)
   - ‚úÖ **Require status checks to pass before merging**
     - ‚úÖ **Require branches to be up to date before merging**
     - **Add required status checks:**
       - `Test CLI Init` (from CI Pipeline)
       - `Shellcheck` (from CI Pipeline)
   - ‚úÖ **Require conversation resolution before merging** (optional but good practice)
   - ‚úÖ **Do not allow bypassing the above settings**
5. Click **Create**

**Result:** All changes to `main` must:
- Come from `dev` via PR
- Pass CI pipeline (linting, tests)

### 5.2 Protect the `dev` Branch

1. Click **Add branch protection rule** again
2. **Branch name pattern:** `dev`
3. Enable:
   - ‚úÖ **Require a pull request before merging** (forces PR from feature branches)
   - ‚ùå **Require approvals** - UNCHECK (allows you to merge your own PRs)
   - ‚úÖ **Require status checks to pass before merging**
     - ‚úÖ **Require branches to be up to date before merging**
     - **Add required status checks:**
       - `Test CLI Init`
       - `Shellcheck`
   - ‚ùå **Do not allow bypassing** - UNCHECK (gives you flexibility on dev)
4. Click **Create**

**Result:** Feature branches must:
- Create PR to `dev` (not directly to `main`)
- Pass CI checks before merging

## Step 6: Configure Notifications

Set up notifications for security alerts:

1. Click on your **profile icon** (top right) ‚Üí **Settings**
2. In the left sidebar, click **Notifications**
3. Scroll down to the **System** section
4. Enable the following:
   - ‚úÖ **Dependabot alerts: New vulnerabilities** - "When you're given access to Dependabot alerts automatically receive notifications when a new vulnerability is found in one of your dependencies."
   - ‚úÖ **Dependabot alerts: Email digest** - "Email a regular summary of Dependabot alerts for up to 10 of your repositories."
   - ‚úÖ **Security campaign emails** - "Receive email notifications about security campaigns in repositories where you have access to security alerts."

**Result:** You'll now receive email notifications whenever security issues are detected in your repositories.

## Step 7: Verify Everything Works

### 7.1 Test CI Pipeline

```bash
# Create a test branch
git checkout -b test/ci-pipeline

# Make a small change
echo "# Test" >> README.md

# Commit and push
git add README.md
git commit -m "test: verify CI pipeline"
git push origin test/ci-pipeline

# Create a PR on GitHub: test/ci-pipeline -> main
# Verify CI pipeline runs and passes
```

### 7.2 Test Dependabot

Dependabot runs weekly, but you can trigger it manually:

1. Go to **Insights** ‚Üí **Dependency graph** ‚Üí **Dependabot**
2. Click **Check for updates**

### 7.3 Test Secret Scanning

If enabled, try pushing a test secret:

```bash
# This should be blocked by local pre-commit hook
echo "API_KEY=test123" > secret.txt
git add secret.txt
git commit -m "test: secret scanning"
# Blocked locally!

# If you bypass local hook with --no-verify, GitHub will block the push
```

## Workflow Files Reference

### `.github/workflows/ci.yml`

**Purpose:** Continuous Integration pipeline

**Triggers:**
- Push to `main` branch
- Pull requests to `main` branch

**Jobs:**
1. **Shellcheck** - Lints shell scripts
2. **Test CLI Init** - Verifies the Forge CLI works

### `.github/dependabot.yml`

**Purpose:** Automated dependency updates

**Configuration:**
- **GitHub Actions ecosystem:** Scans workflow files
  - Schedule: Weekly
  - Groups updates

## Troubleshooting

### Workflows Not Appearing in Actions Tab

**Symptoms:** Actions tab shows "Get started with GitHub Actions" instead of workflows

**Causes:**
1. Workflow files not committed/pushed
2. Workflow files in wrong directory
3. YAML syntax errors
4. GitHub Actions disabled in repo settings

**Solutions:**
```bash
# 1. Verify files are committed
git ls-files .github/workflows/

# 2. Verify files are pushed
git log --oneline --name-only | grep workflows

# 3. Validate YAML syntax
npx js-yaml .github/workflows/ci.yml

# 4. Check repo settings
# Go to Settings ‚Üí Actions ‚Üí General ‚Üí Verify "Allow all actions" is selected
```

## Security Best Practices

1. **Enable all security features:**
   - ‚úÖ Dependabot alerts
   - ‚úÖ Secret scanning
   - ‚úÖ Push protection

2. **Protect main branch:**
   - Require PR reviews
   - Require status checks to pass
   - Prevent force pushes

3. **Use local pre-commit hooks:**
   - Catch secrets before pushing
   - Enforce code quality locally
   - Faster feedback loop

## Related Documentation

- [CI/CD Pipeline Documentation](./overview.md)
- [Git Workflow Guide](./git_workflow.md)
- [How to Commit Guide](./how_to_commit.md)
- [ADR-039: CI/CD Pipeline Strategy](../../docs/adr/039_ci_cd_pipeline.md)
- [ADR-040: Security Scanning Strategy](../../docs/adr/040_security_scanning_strategy.md)
- [Manual Setup Guide](../../TASKS/active/0067_MANUAL_SETUP_GUIDE.md)

## External Resources

- [GitHub Actions Documentation](https://docs.github.com/en/actions)
- [Dependabot Documentation](https://docs.github.com/en/code-security/dependabot)
- [Secret Scanning Documentation](https://docs.github.com/en/code-security/secret-scanning)

```

## File: guides/cicd/how_to_commit.md
```md
# How to Commit Changes - Step-by-Step Guide

This guide walks you through committing changes to the quantum-diamond-forge project, including pre-commit hook validation and conventional commit format.

## Prerequisites

- Git configured with hooks path: `git config core.hooksPath .githooks`
- Pre-commit hook is executable: `chmod +x .githooks/pre-commit`

## Standard Commit Workflow

### Step 1: Check Current Status

```bash
# See what files have changed
git status

# See detailed changes
git diff
```

### Step 2: Stage Files

**Option A: Stage specific files (recommended)**
```bash
git add path/to/file1.js
git add path/to/file2.md
git add path/to/file3.yml
```

**Option B: Stage all changes**
```bash
git add .
# or
git add --all
```

**Option C: Interactive staging (stage specific lines)**
```bash
git add -p
# Git will show each change and ask: Stage this hunk [y,n,q,a,d,e,?]?
# y = yes, n = no, q = quit, a = all, d = don't stage, e = edit
```

### Step 3: Review Staged Changes

**Quick summary (recommended):**
```bash
# See list of staged files
git status

# Even shorter
git status -s
```

**Detailed diff (optional):**
```bash
# See detailed changes (can be verbose)
git diff --cached

# Press 'q' to exit the diff view
```

**‚ö†Ô∏è IMPORTANT:** Always review your staged changes before committing!

### Step 4: Commit with Conventional Format

```bash
git commit -m "<type>(<scope>): <subject>

<body>

<footer>"
```

**Commit Types:**
- `feat:` - New feature
- `fix:` - Bug fix
- `docs:` - Documentation changes
- `style:` - Code formatting (no logic change)
- `refactor:` - Code restructuring
- `test:` - Adding/updating tests
- `chore:` - Maintenance tasks (dependencies, build)
- `ci:` - CI/CD changes
- `perf:` - Performance improvements

**Example:**
```bash
git commit -m "feat(security): configure GitHub Advanced Security

- Add Dependabot for dependency scanning
- Add CodeQL workflow for security analysis
- Update CI/CD documentation with security guide

Refs: TASK-0067, ADR-040"
```

### Step 5: Pre-commit Hook Validation

**What happens automatically:**
1. ‚úÖ Hook runs: `.githooks/pre-commit`
2. ‚úÖ Validates no `.env` files (except `.env.example`)
3. ‚úÖ Scans for hardcoded secrets (API keys, tokens, passwords)
4. ‚úÖ If validation passes ‚Üí commit succeeds
5. ‚ùå If violations found ‚Üí commit blocked

**If commit is blocked:**
```bash
# Example error:
COMMIT BLOCKED: Violations found.
VIOLATION: packages/backend/config.js:12 -> OPENAI_API_KEY=<REDACTED>
Fix by removing secrets or using '<REDACTED>'.

# Fix the issue:
# 1. Remove the hardcoded secret
# 2. Use environment variable instead: process.env.OPENAI_API_KEY
# 3. Try committing again
```

**Bypass hook (ONLY if absolutely necessary):**
```bash
git commit --no-verify -m "your message"
# ‚ö†Ô∏è WARNING: Only use --no-verify if you're certain there are no secrets!
```

### Step 6: Push to Remote

```bash
# Push to current branch
git push

# Push to specific branch
git push origin feature/branch-name

# Push to main
git push origin main
```

## Example: Committing TASK-0067 Security Configuration

```bash
# 1. Check status
git status

# 2. Stage security configuration files
git add .github/dependabot.yml
git add .github/workflows/codeql.yml
git add docs/ci-cd/README.md
git add docs/ci-cd/GIT_WORKFLOW.md
git add docs/ci-cd/HOW_TO_COMMIT.md
git add adrs/040_security_scanning_strategy.md
git add adrs/041_git_workflow_automation.md
git add TASKS/in-progress/008_configure_github_security.md
git add TASKS/backlog/009_enhance_precommit_hooks.md
git add scripts/capture_snapshot.js

# Note: Deleted file (.githooks/pre-commit.sh) will be automatically staged
# when you run 'git add .' or will show in 'git status' as deleted

# 3. Review staged changes (quick summary)
git status

# Or see detailed diff (verbose, press 'q' to exit)
# git diff --cached

# 4. Commit with conventional format
git commit -m "feat(security): configure GitHub Advanced Security

- Add Dependabot for npm and GitHub Actions dependency scanning
- Add CodeQL workflow for JavaScript/TypeScript security analysis
- Update CI/CD documentation with comprehensive security scanning guide
- Create ADR-041 for git workflow automation strategy
- Create git workflow quick reference guide
- Enhance snapshot script to exclude agents/feedback directory
- Remove deprecated pre-commit.sh shell script

Deliverables:
- .github/dependabot.yml (weekly scans, grouped PRs)
- .github/workflows/codeql.yml (security-extended queries)
- docs/ci-cd/README.md (175-line security guide, pre-commit hook docs)
- docs/ci-cd/GIT_WORKFLOW.md (conventional commits, aliases, best practices)
- adrs/041_git_workflow_automation.md (no automated git scripts)
- TASKS/backlog/009_enhance_precommit_hooks.md (future ESLint/Prettier integration)

Refs: TASK-0067, ADR-040, ADR-041"

# 5. Pre-commit hook runs automatically (validates no secrets)

# 6. Push to GitHub
git push origin main
```

## Testing Pre-commit Hook

### Test 1: Verify Hook Blocks Secrets

```bash
# Create a test file with a hardcoded secret
echo "OPENAI_API_KEY=<REDACTED>" > test-secret.txt

# Try to commit (should be BLOCKED)
git add test-secret.txt
git commit -m "test: verify pre-commit hook blocks secrets"

# Expected output:
# COMMIT BLOCKED: Violations found.
# VIOLATION: test-secret.txt:1 -> OPENAI_API_KEY=<REDACTED>
# Fix by removing secrets or using '<REDACTED>'.

# Clean up
git reset HEAD test-secret.txt
rm test-secret.txt
```

### Test 2: Verify Hook Blocks .env Files

```bash
# Create a .env file
echo "DATABASE_URL=postgres://localhost" > .env

# Try to commit (should be BLOCKED)
git add .env
git commit -m "test: verify pre-commit hook blocks .env files"

# Expected output:
# COMMIT BLOCKED: Violations found.
# BLOCKED .env file: .env
# Fix by removing secrets or using '<REDACTED>'.

# Clean up
git reset HEAD .env
rm .env
```

### Test 3: Verify Hook Allows Safe Code

```bash
# Create a safe file with environment variable reference
echo "const apiKey = process.env.OPENAI_API_KEY;" > test-safe.js

# Commit (should SUCCEED)
git add test-safe.js
git commit -m "test: verify pre-commit hook allows safe code"

# Expected: Commit succeeds (no violations)

# Clean up
git reset HEAD~1  # Undo last commit
rm test-safe.js
```

## Common Issues and Solutions

### Issue 1: Pre-commit Hook Not Running

**Symptoms:** Commits succeed without validation

**Solution:**
```bash
# Verify hooks path is configured
git config core.hooksPath
# Should output: .githooks

# If not set, configure it
git config core.hooksPath .githooks

# Make hook executable
chmod +x .githooks/pre-commit

# Verify hook exists
ls -la .githooks/pre-commit
```

### Issue 2: Hook Blocks Legitimate Code

**Symptoms:** Hook blocks code that uses environment variables

**Example:**
```javascript
// This might be flagged if not properly formatted
const key = API_KEY;  // ‚ùå Flagged (looks like hardcoded value)

// Use these patterns instead:
const key = process.env.API_KEY;  // ‚úÖ Safe
const key = import.meta.env.VITE_API_KEY;  // ‚úÖ Safe
const key = config.apiKey;  // ‚úÖ Safe
```

**Solution:** Use whitelisted patterns (see `.githooks/pre-commit` for full list)

### Issue 3: Accidentally Committed Secret

**‚ö†Ô∏è CRITICAL - Act Immediately:**

```bash
# 1. IMMEDIATELY revoke the secret in the service provider
# (e.g., regenerate API key in OpenAI dashboard)

# 2. Remove from git history (if not yet pushed)
git reset HEAD~1  # Undo last commit
# Fix the file, then commit again

# 3. If already pushed, use git filter-branch
git filter-branch --force --index-filter \
  "git rm --cached --ignore-unmatch path/to/file" \
  --prune-empty --tag-name-filter cat -- --all

# 4. Force push (‚ö†Ô∏è coordinate with team!)
git push origin --force --all

# 5. Update environment variables with new secret
```

## Git Aliases (Optional Shortcuts)

Add these to `~/.gitconfig` for faster workflows:

```gitconfig
[alias]
    # Quick status
    st = status -sb

    # Stage all changes
    aa = add --all

    # Commit with message
    cm = commit -m

    # Show staged changes
    staged = diff --cached

    # Amend last commit
    amend = commit --amend --no-edit

    # Pretty log
    lg = log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit
```

**Usage:**
```bash
git st              # Instead of: git status -sb
git aa              # Instead of: git add --all
git staged          # Instead of: git diff --cached
git cm "fix: typo"  # Instead of: git commit -m "fix: typo"
```

## Best Practices

1. **Commit often** - Small, focused commits are easier to review and revert
2. **Write clear messages** - Use conventional commit format
3. **Review before committing** - Always run `git diff --cached`
4. **Test locally** - Run `npm run lint` and `npm run test:unit` before committing
5. **Never bypass hooks** - Only use `--no-verify` in emergencies
6. **Keep commits atomic** - One logical change per commit
7. **Reference tasks/issues** - Include `Refs: TASK-XXX` in commit body

## After Pushing to GitHub

Once you push, the following automated checks will run:

1. **CI Pipeline** (`.github/workflows/ci.yml`)
   - Linting
   - Unit tests
   - Frontend build

2. **CodeQL Analysis** (`.github/workflows/codeql.yml`)
   - Security vulnerability scanning
   - Results in Security tab

3. **Dependabot** (`.github/dependabot.yml`)
   - Dependency vulnerability scanning
   - Automatic PRs for updates

4. **Secret Scanning** (if enabled)
   - Detects committed secrets
   - Alerts in Security tab

Check the **Actions** tab and **Security** tab on GitHub to verify all checks pass.

## References

- [Git Workflow Quick Reference](./git_workflow.md)
- [CI/CD Pipeline Documentation](./overview.md)
- [ADR-041: Git Workflow Automation](../../docs/adr/041_git_workflow_automation.md)
- [Conventional Commits Specification](https://www.conventionalcommits.org/)

```

## File: guides/cicd/overview.md
```md
# CI/CD Pipeline & Development Workflow

## Overview

This document outlines the Continuous Integration (CI) pipeline and the standard development workflow for projects built with the **Quantum Diamond Forge** protocol. It details the lifecycle of a code change from a developer's workstation to the main branch on GitHub.

## Table of Contents

1. [Workflow Diagram](#workflow-diagram)
2. [Development Workflow Phases](#development-workflow-phases)
3. [Security Scanning Results Guide](#security-scanning-results-guide)
4. [How to Commit Changes](./how_to_commit.md) - Step-by-step commit guide with pre-commit hook testing
5. [Git Workflow Quick Reference](./git_workflow.md) - Conventional commits, aliases, and best practices
6. [Related Documentation](#related-documentation)

## Related Documentation

- **[GitHub Repository Setup Guide](./github_setup.md)** - Configure GitHub Actions, security scanning, and branch protection
- **[How to Commit Changes](./how_to_commit.md)** - Step-by-step commit guide with pre-commit hook testing
- **[Git Workflow Guide](./git_workflow.md)** - Detailed guide on git commands, conventional commits, and pre-commit hooks
- **[ADR-039: CI/CD Pipeline Strategy](../../docs/adr/039_ci_cd_pipeline.md)** - Architectural decision for CI/CD approach
- **[ADR-040: Security Scanning Strategy](../../docs/adr/040_security_scanning_strategy.md)** - Security scanning tools and philosophy
- **[ADR-041: Git Workflow Automation](../../docs/adr/041_git_workflow_automation.md)** - Git workflow best practices

## Branching Strategy

This protocol supports **flexible branching strategies** based on team size:

### Solo Developer (Simplified)
```
feature/* ‚Üí main (via Pull Request)
```

### Team / Staged Releases (Recommended)
```
feature/* ‚Üí dev ‚Üí main
```

### Enterprise / Multi-Environment
```
feature/* ‚Üí dev ‚Üí test ‚Üí main
```

### Branch Purposes

| Branch | Purpose | CI Runs | Deployment |
|--------|---------|---------|------------|
| `feature/*` | Active development | ‚úÖ On PR | None |
| `dev` | Integration testing, batch features | ‚úÖ On push/PR | Dev environment (optional) |
| `test` | QA/staging (optional) | ‚úÖ On push/PR | Test environment (optional) |
| `main` | Production-ready | ‚úÖ On push/PR | Production |

### Workflow (Team / Staged Releases)

1. **Feature Development:**
   ```bash
   git checkout -b feature/add-new-feature
   # Make changes, commit, push
   git push origin feature/add-new-feature
   # Create PR: feature/add-new-feature ‚Üí dev
   ```

2. **Integration Testing (dev):**
   - Merge feature PRs into `dev`
   - CI pipeline runs automatically
   - Test integration with other features
   - Batch multiple features for the next release

3. **Production Release (main):**
   ```bash
   # Create PR: dev ‚Üí main
   # After approval and CI passes, merge
   # Tag release: git tag v1.0.0 && git push --tags
   ```

### Branch Protection

Recommended protection for `dev` and `main`:
- ‚úÖ CI pipeline checks (linting, tests, build)
- ‚úÖ CodeQL security analysis (if enabled)
- ‚úÖ PR review required (for `main`, optional for `dev`)
- ‚úÖ Status checks must pass before merge

See [GitHub Repository Setup Guide](./github_setup.md) for configuration details.

## Workflow Diagram

The following sequence diagram illustrates the interaction between the Developer, their Local Workstation, and the specific entities within GitHub (Branches, PRs, CI).

```mermaid
---
config:
  theme: base
---
sequenceDiagram
    autonumber
    participant Dev as Developer
    participant Local as Local Workstation
    participant FeatBranch as Remote Feature Branch
    participant PR as Pull Request
    participant CI as GitHub Actions (CI)
    participant MainBranch as Remote Main Branch

    Note over Dev, Local: 1. Feature Start
    Dev->>Local: git checkout -b feature/new-feature

    Note over Dev, Local: 2. Development Loop
    loop Coding & Local Testing
        Dev->>Local: Write Code
        Dev->>Local: npm run lint (Check Style)
        Dev->>Local: npm run test:unit (Verify Logic)
        Dev->>Local: (Optional) Manual Security Scan
    end

    Note over Dev, Local: 3. Commit & Push (Defense in Depth)
    Dev->>Local: git add .
    Local->>Local: Pre-commit Hook (Secret Detection)
    Note right of Local: üõë Blocking Gate:<br/>- No .env files<br/>- No hardcoded secrets<br/>- Blocks commit if violations found

    Dev->>Local: git commit -m "feat: add new feature"

    rect rgb(255, 255, 240)
        Note right of Local: ‚ö†Ô∏è Post-Commit Hook (Informational):<br/>- Auto-runs 'npm audit' (High Severity)<br/>- Checks local Dependabot status<br/>- Warns Dev immediately (does not block)
        Local-->>Dev: Display "Security Health Report"
    end

    Dev->>Local: git push -u origin feature/new-feature
    Local->>FeatBranch: Create/Update Branch

    Note over Dev, PR: 4. Pull Request
    Dev->>PR: Create PR (Feature -> Main)

    Note over PR, CI: 5. Automated Checks
    PR->>CI: Trigger "CI Pipeline" Workflow

    par CI Pipeline
        rect rgb(240, 248, 255)
            Note right of CI: CI Execution
            CI->>CI: Checkout Code
            CI->>CI: Install Dependencies
            CI->>CI: Linting & Tests
            CI->>CI: Build Frontend
        end
        CI-->>PR: Report Status (‚úÖ/‚ùå)
    and Security Checks
        rect rgb(255, 240, 245)
            Note right of PR: GitHub Security
            PR->>PR: Dependabot Scan
            PR->>PR: CodeQL Analysis
            PR->>PR: Secret Scanning
        end
        PR-->>Dev: Report Vulnerabilities (in PR Interface)
    end

    alt Checks Fail
        PR-->>Dev: Notify Failure
        Dev->>Local: Fix Code & Push Again
        Local->>FeatBranch: Update Branch
        FeatBranch->>PR: Update PR
        PR->>CI: Re-trigger CI
    else Checks Pass
        Note over Dev, MainBranch: 6. Review & Merge
        Dev->>PR: Request Review
        PR->>MainBranch: Merge PR to 'main'

        Note over MainBranch, FeatBranch: 7. Cleanup
        MainBranch->>FeatBranch: Delete Remote Branch
    end

    Note over Dev, Local: 8. Local Cleanup
    Dev->>Local: git checkout main
    Dev->>Local: git pull origin main
    Local->>MainBranch: Fetch Latest
    Dev->>Local: git branch -d feature/new-feature
```

## Detailed Workflow Steps

### Phase 1: Developer Workstation (Local)

1.  **Create Feature Branch**
    *   **Command:** `git checkout -b feature/<name>`
    *   **Purpose:** Isolate changes from the stable `main` codebase.

2.  **Development & Verification**
    *   **Process:** Write code, update tests.
    *   **Verification:**
    *   Run `npm run lint` and `npm run test:unit`.
    *   **Post-Commit Hook:** After commit, an informational `npm audit --audit-level=high --production` runs and displays a Security Health Report (does not block).

3.  **Commit & Push**
    *   **Command:** `git commit` and `git push`.
    *   **Pre-commit Hook:** Automatically runs `.githooks/pre-commit` to validate:
        *   No `.env` files committed (except `.env.example`)
        *   No hardcoded secrets (API keys, tokens, passwords)
        *   Blocks commit if violations found
    *   **Entity:** Updates the **Remote Feature Branch** (`origin/feature/<name>`).
    *   **Best Practice:** Use [conventional commits](./GIT_WORKFLOW.md#conventional-commit-format) (e.g., `feat:`, `fix:`, `docs:`)

### Phase 2: GitHub (Remote)

4.  **Create Pull Request (PR)**
    *   **Action:** Create a PR merging **Remote Feature Branch** into **Remote Main Branch**.
    *   **Purpose:** This is the central hub for review and automated checks.

5.  **Automated Checks**
    *   **CI Pipeline:** GitHub Actions runs linting, testing, and building. Reports success/failure back to the PR.
    *   **Security Scans:**
        *   **Dependabot:** Scans dependencies for vulnerabilities. If found, it alerts in the PR or creates a new PR.
        *   **Secret Scanning:** Checks for committed secrets (API keys, tokens).
        *   **CodeQL:** (If enabled) Performs static analysis for security flaws.
    *   **Reporting:** All results are displayed in the "Checks" section of the PR interface.


6.  **Code Review & Merge**
    *   **Action:** If all checks pass (Green ‚úÖ), the PR is merged.
    *   **Result:** Code moves from **Remote Feature Branch** to **Remote Main Branch**.

7.  **Remote Cleanup**
    *   **Action:** The **Remote Feature Branch** is deleted to keep the repository clean.

### Phase 3: Developer Workstation (Local Cleanup)

8.  **Sync & Cleanup**
    *   **Action:** Pull the latest `main` from **Remote Main Branch** and delete the local feature branch.

---

## Security Scanning Results Guide

This section explains how to interpret and respond to security scanning results from our automated tools.

### Dependabot Alerts

**What it does:** Scans `package.json` and `package-lock.json` for known vulnerabilities in dependencies.

**Where to find results:**
- **Security tab** ‚Üí Dependabot alerts
- **Pull Requests** ‚Üí Dependabot automatically opens PRs for updates

**How to interpret:**
- **Critical/High:** Address immediately (within 48 hours)
- **Medium:** Address within 1 week
- **Low:** Address during regular maintenance

**Response actions:**
1. Review the Dependabot PR description for vulnerability details
2. Check if the update includes breaking changes (review CHANGELOG)
3. Verify tests pass in the Dependabot PR
4. Merge the PR or manually update the dependency
5. If update causes issues, document in PR and investigate alternatives

**Example Dependabot PR:**
```
Title: Bump axios from 0.21.1 to 1.6.0
Labels: dependencies, security

Description:
- Fixes CVE-2023-45857 (High severity)
- Changelog: https://github.com/axios/axios/releases
```

### CodeQL Analysis

**What it does:** Static code analysis to detect security vulnerabilities (SQL injection, XSS, path traversal, etc.)

**Where to find results:**
- **Security tab** ‚Üí Code scanning alerts
- **Pull Request checks** ‚Üí CodeQL analysis status

**How to interpret:**
- **Error:** Security vulnerability detected, must fix before merge
- **Warning:** Potential issue, review and address if applicable
- **Note:** Informational, no action required

**Common alerts:**
- **Unvalidated user input:** Always validate/sanitize user input
- **SQL injection:** Use parameterized queries (we use Supabase client, which handles this)
- **XSS vulnerabilities:** Sanitize output, use React's built-in XSS protection
- **Path traversal:** Validate file paths before file operations
- **Hardcoded credentials:** Never commit secrets (use environment variables)

**Response actions:**
1. Click on the alert in the Security tab to see details
2. Review the code path highlighted by CodeQL
3. Determine if it's a true positive or false positive
4. If true positive: Fix the vulnerability and push a new commit
5. If false positive: Document why it's safe and dismiss the alert with justification

**Example CodeQL alert:**
```
Alert: Unvalidated user input in file path
Severity: High
File: packages/backend/api/controllers/fileController.js:45
Recommendation: Validate and sanitize the file path before use
```

### Secret Scanning

**What it does:** Detects accidentally committed secrets (API keys, tokens, passwords)

**Where to find results:**
- **Security tab** ‚Üí Secret scanning alerts
- **Push protection:** Blocks commits containing secrets (if enabled)

**How to interpret:**
- **Active:** Secret is currently in the repository
- **Resolved:** Secret has been removed or revoked

**Response actions (CRITICAL - Act immediately):**
1. **Revoke the exposed secret** in the service provider (e.g., regenerate API key)
2. **Remove the secret from git history** (use `git filter-branch` or BFG Repo-Cleaner)
3. **Update environment variables** with the new secret
4. **Verify the secret is not in any commits** (check git log)
5. **Document the incident** and review how it happened

**Prevention:**
- Use `.env` files (already in `.gitignore`)
- Store secrets in user profile (`~/.zshrc` or `~/.bashrc`)
- Use `npm audit` locally before committing
- Enable push protection in GitHub settings

**Example secret scanning alert:**
```
Alert: GitHub Personal Access Token detected
File: packages/backend/.env
Commit: abc123def456
Status: Active
Action Required: Revoke token immediately
```

### Local Security Checks

**Before every commit, run:**
```bash
# Check for vulnerable dependencies
npm audit

# Fix automatically fixable vulnerabilities
npm audit fix

# Review high-severity vulnerabilities
npm audit --audit-level=high
```

**Interpreting `npm audit` output:**
```
found 3 vulnerabilities (1 moderate, 2 high)

Moderate: Prototype Pollution in lodash
  Package: lodash
  Patched in: >=4.17.21
  Fix available: npm audit fix

High: Regular Expression Denial of Service in semver
  Package: semver
  Patched in: >=7.5.2
  Fix available: npm audit fix
```

**Response:**
- Run `npm audit fix` to auto-fix
- If auto-fix not available, manually update the package
- If no fix available, assess risk and consider alternatives

### Security Check Status in PRs

All PRs must pass these checks before merge:

| Check | Status | Action if Failed |
|-------|--------|------------------|
| **CI Pipeline** | ‚úÖ Must pass | Fix linting/test errors |
| **CodeQL** | ‚úÖ Must pass | Fix security vulnerabilities |
| **Dependabot** | ‚ö†Ô∏è Advisory | Review and merge dependency updates |
| **Secret Scanning** | üö® Must pass | Revoke and remove secrets immediately |

**Green ‚úÖ = Safe to merge**
**Yellow ‚ö†Ô∏è = Review required**
**Red üö® = Blocking issue, must fix**

### Escalation Path

If you encounter a security issue you're unsure how to handle:

1. **Do not merge the PR**
2. **Tag the issue** with `security` label
3. **Document the issue** in the PR comments
4. **Consult ADR-040** for security scanning strategy
5. **Reach out** to the team lead or security contact

### Additional Resources

- [GitHub Security Best Practices](https://docs.github.com/en/code-security)
- [OWASP Top 10](https://owasp.org/www-project-top-ten/)
- [npm audit documentation](https://docs.npmjs.com/cli/v8/commands/npm-audit)
- ADR-040: Security Scanning Strategy

```

## File: guides/cicd/security_scanning.md
```md
# Security Vulnerability Scanning Guide

## Overview

This guide covers how to scan for security vulnerabilities in your dependencies using GitHub CLI and integrate security scanning into your shift-left development process.

## Table of Contents

- [Prerequisites](#prerequisites)
- [Scanning with GitHub CLI](#scanning-with-github-cli)
- [Understanding Dependabot Alerts](#understanding-dependabot-alerts)
- [Shift-Left Security Integration](#shift-left-security-integration)
- [Local Security Scanning](#local-security-scanning)
- [Automated Workflows](#automated-workflows)
- [Best Practices](#best-practices)

## Prerequisites

### Install GitHub CLI

```bash
# macOS
brew install gh

# Authenticate with GitHub
gh auth login
```

### Required Permissions

Ensure your GitHub token has the following scopes:
- `repo` - Full control of private repositories
- `read:org` - Read org and team membership
- `workflow` - Update GitHub Action workflows

## Scanning with GitHub CLI

### Check Authentication Status

```bash
gh auth status
```

### View All Dependabot Alerts

```bash
# List all alerts
gh api repos/OWNER/REPO/dependabot/alerts

# Pretty formatted output
gh api repos/richfrem/ingPoC/dependabot/alerts \
  --jq '.[] | {
    number: .number,
    severity: .security_advisory.severity,
    package: .dependency.package.name,
    summary: .security_advisory.summary,
    patched_version: .security_advisory.vulnerabilities[0].first_patched_version.identifier
  }'
```

### Filter by Severity

```bash
# High severity only
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts \
  --jq '.[] | select(.security_advisory.severity == "high") | {
    package: .dependency.package.name,
    summary: .security_advisory.summary
  }'

# Critical and High severity
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts \
  --jq '.[] | select(.security_advisory.severity == "critical" or .security_advisory.severity == "high")'
```

### Count Open Alerts

```bash
# Total open alerts
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts \
  --jq '[.[] | select(.state == "open")] | length'

# By severity
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts \
  --jq 'group_by(.security_advisory.severity) | map({severity: .[0].security_advisory.severity, count: length})'
```

### Get Detailed Alert Information

```bash
# Get specific alert details
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts/ALERT_NUMBER

# Get fix recommendations
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts \
  --jq '.[] | {
    package: .dependency.package.name,
    current_version: .dependency.package.version,
    patched_version: .security_advisory.vulnerabilities[0].first_patched_version.identifier,
    cvss_score: .security_advisory.cvss.score
  }'
```

## Understanding Dependabot Alerts

### Alert Severity Levels

- **Critical**: Immediate action required (CVSS 9.0-10.0)
- **High**: Should be addressed quickly (CVSS 7.0-8.9)
- **Medium**: Address in normal development cycle (CVSS 4.0-6.9)
- **Low**: Address when convenient (CVSS 0.1-3.9)

### Alert States

- **open**: Vulnerability is present and unresolved
- **dismissed**: Manually dismissed by a user
- **fixed**: Dependency has been updated to a non-vulnerable version

## Shift-Left Security Integration

### Why Shift-Left Security?

Shift-left security means integrating security checks **earlier** in the development process:

‚úÖ **Benefits:**
- Catch vulnerabilities before they reach production
- Reduce cost of fixes (cheaper to fix in development)
- Faster feedback loop for developers
- Prevent vulnerable code from being committed

‚ùå **Without Shift-Left:**
- Vulnerabilities discovered in production
- Emergency patches and hotfixes
- Potential security incidents
- Higher remediation costs

### Pre-Commit Security Checks

Yes, you **should** integrate security scanning into your pre-commit process! Here's how:

## Local Security Scanning

### 1. NPM Audit (Built-in)

```bash
# Run npm audit
npm audit

# Get JSON output
npm audit --json

# Fix automatically (use with caution)
npm audit fix

# Fix only production dependencies
npm audit fix --production-only

# Dry run to see what would be fixed
npm audit fix --dry-run
```

### 2. Create a Pre-Commit Security Check

Add to your `package.json`:

```json
{
  "scripts": {
    "security:check": "npm audit --audit-level=high",
    "security:fix": "npm audit fix",
    "precommit:security": "npm audit --audit-level=critical --production"
  }
}
```

### 3. Integrate with Husky/lint-staged

Update your `.husky/pre-commit` or create one:

```bash
#!/usr/bin/env sh
. "$(dirname -- "$0")/_/husky.sh"

# Run security audit before commit
echo "üîí Running security audit..."
npm audit --audit-level=high --production

if [ $? -ne 0 ]; then
  echo "‚ùå Security vulnerabilities found! Please fix before committing."
  echo "Run 'npm audit' for details or 'npm audit fix' to attempt automatic fixes."
  exit 1
fi

# Continue with other pre-commit checks
npx lint-staged
```

### 4. Alternative: Use Snyk CLI

Snyk provides more comprehensive scanning:

```bash
# Install Snyk
npm install -g snyk

# Authenticate
snyk auth

# Test for vulnerabilities
snyk test

# Monitor project (sends results to Snyk dashboard)
snyk monitor

# Test and fail on high severity
snyk test --severity-threshold=high
```

### 5. GitHub CLI Pre-Push Check

Create a script to check before pushing:

```bash
#!/bin/bash
# .git/hooks/pre-push or scripts/pre-push-security.sh

echo "üîç Checking for Dependabot alerts..."

OPEN_ALERTS=$(gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts \
  --jq '[.[] | select(.state == "open" and (.security_advisory.severity == "critical" or .security_advisory.severity == "high"))] | length')

if [ "$OPEN_ALERTS" -gt 0 ]; then
  echo "‚ö†Ô∏è  Warning: $OPEN_ALERTS critical/high severity alerts found in GitHub!"
  echo "Run: gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts --jq '.[] | select(.state == \"open\")' for details"

  read -p "Continue with push? (y/n) " -n 1 -r
  echo
  if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    exit 1
  fi
fi
```

## Automated Workflows

### Recommended Shift-Left Security Strategy

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Development Workflow                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  1. Local Development                                        ‚îÇ
‚îÇ     ‚îî‚îÄ> npm audit (manual check)                            ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  2. Pre-Commit Hook                                          ‚îÇ
‚îÇ     ‚îî‚îÄ> npm audit --audit-level=high                        ‚îÇ
‚îÇ     ‚îî‚îÄ> Fail on critical/high vulnerabilities               ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  3. Pre-Push Hook (Optional)                                 ‚îÇ
‚îÇ     ‚îî‚îÄ> Check GitHub Dependabot alerts via CLI              ‚îÇ
‚îÇ     ‚îî‚îÄ> Warn on open critical/high alerts                   ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  4. CI/CD Pipeline                                           ‚îÇ
‚îÇ     ‚îî‚îÄ> npm audit in GitHub Actions                         ‚îÇ
‚îÇ     ‚îî‚îÄ> Dependabot auto-updates                             ‚îÇ
‚îÇ     ‚îî‚îÄ> SAST/DAST scanning                                  ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  5. Production Monitoring                                    ‚îÇ
‚îÇ     ‚îî‚îÄ> Continuous Dependabot monitoring                    ‚îÇ
‚îÇ     ‚îî‚îÄ> Security alerts via GitHub                          ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Add to package.json Scripts

```json
{
  "scripts": {
    "security:audit": "npm audit",
    "security:audit:ci": "npm audit --audit-level=moderate --production",
    "security:fix": "npm audit fix",
    "security:check:github": "gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts --jq '.[] | select(.state == \"open\")'",
    "precommit": "npm run security:audit && lint-staged"
  }
}
```

### GitHub Actions Workflow

Create `.github/workflows/security-scan.yml`:

```yaml
name: Security Scan

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  security-audit:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run npm audit
        run: npm audit --audit-level=moderate
        continue-on-error: true

      - name: Check Dependabot alerts
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh api repos/${{ github.repository }}/dependabot/alerts \
            --jq '.[] | select(.state == "open") | {severity: .security_advisory.severity, package: .dependency.package.name}'
```

## Best Practices

### 1. **Regular Scanning**
- Run `npm audit` before every commit
- Check Dependabot alerts weekly
- Review security advisories for your dependencies

### 2. **Prioritize Fixes**
- **Critical/High**: Fix immediately
- **Medium**: Fix within sprint
- **Low**: Fix during maintenance windows

### 3. **Keep Dependencies Updated**
```bash
# Check for outdated packages
npm outdated

# Update to latest within semver range
npm update

# Update to latest (breaking changes possible)
npm install package@latest
```

### 4. **Use Dependabot Auto-Updates**

Enable in `.github/dependabot.yml`:

```yaml
version: 2
updates:
  - package-ecosystem: "npm"
    directory: "/"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 10
    reviewers:
      - "your-team"
    labels:
      - "dependencies"
      - "security"
```

### 5. **Monitor Production**
- Enable GitHub security alerts
- Set up Slack/email notifications for new vulnerabilities
- Use GitHub Security Advisory Database

### 6. **Document Exceptions**
If you must dismiss an alert:
```bash
# Dismiss with reason
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts/ALERT_NUMBER \
  -X PATCH \
  -f state=dismissed \
  -f dismissed_reason=no_bandwidth \
  -f dismissed_comment="Will address in Q2 security sprint"
```

## Quick Reference Commands

```bash
# Check auth
gh auth status

# List all open alerts
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts --jq '.[] | select(.state == "open")'

# Count by severity
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts --jq 'group_by(.security_advisory.severity) | map({severity: .[0].security_advisory.severity, count: length})'

# Local audit
npm audit

# Fix vulnerabilities
npm audit fix

# Production-only audit
npm audit --production

# Fail on high severity
npm audit --audit-level=high
```

## Recommended Pre-Commit Setup

**Balanced Approach** (Recommended for this project):

```bash
# Add to package.json
"scripts": {
  "precommit:security": "npm audit --audit-level=high --production"
}
```

**Why this approach?**
- ‚úÖ Catches critical and high severity issues
- ‚úÖ Only checks production dependencies (dev deps less critical)
- ‚úÖ Fast enough for pre-commit hook
- ‚úÖ Prevents vulnerable code from being committed
- ‚ùå Won't block on medium/low severity issues

## Conclusion

**Should you scan before pushing?**

**Yes!** Implement a **layered security approach**:

1. **Pre-commit**: Fast `npm audit` for critical/high severity
2. **Pre-push**: Optional GitHub Dependabot check (can be slow)
3. **CI/CD**: Comprehensive scanning in GitHub Actions
4. **Continuous**: Dependabot monitoring and auto-PRs

This shift-left approach catches vulnerabilities early, reduces security debt, and maintains a secure codebase without significantly slowing down development.

---

**Last Updated**: 2025-11-21
**Maintained By**: Development Team

```

## File: prompts/00_MASTER_PROTOCOL.md
```md
# Quantum Diamond Forge - Master Protocol (Enterprise Mode)

## Before You Start: Choose Your Mode

**You are viewing: Enterprise Mode** (Full rigor, 25+ artifacts, 4-6 hours)

**Other modes available:**
- **Ultra-Lean Mode**: 5 artifacts, ~30 min ‚Üí `prompts/00_MASTER_PROTOCOL_ULTRA_LEAN.md`
- **Lean Mode**: ~10 artifacts, ~1 hour ‚Üí `prompts/00_MASTER_PROTOCOL_LEAN.md`

**Use Enterprise Mode if:**
- Building production applications
- Working with a team
- Compliance/regulatory requirements
- Need full documentation

**Consider a lighter mode if:**
- Solo developer or small team
- Prototype or MVP
- Time-constrained project
- Already know the domain well

---

## Philosophy
**Objective:**
Generate "Enterprise Benchmark" quality software specifications and code by orchestrating a "Human-in-the-Loop" workflow with high-end LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, or Grok).

**The Concept:**
Instead of relying on a swarm of autonomous agents, **YOU + ANTIGRAVITY** are the orchestrator.
*   **You (The User):** Provide the vision and strategic decisions.
*   **Web LLM (Gemini/Claude):** The "Heavy Lifter" that generates rigorous specs from our prompts.
*   **Antigravity (IDE Agent):** The "Builder" that implements the code based on those specs.

This "Human-in-the-Loop" workflow ensures enterprise-grade quality without losing control.

---

## Phase 1: The Specification Loop

**Goal:** Create a "Gold Standard" documentation suite (`docs/`) that leaves no ambiguity for implementation.

### Step 1: Requirements Analysis
1.  Open `prompts/01_requirements_analysis.md`.
2.  Fill in your "Rough Idea" at the top.
3.  Paste the entire file into your Web LLM.
4.  **Review the Output:** Ensure it has a Feature Catalog (MoSCoW), Functional Requirements, and User Stories.
5.  **Save:** Create `docs/01_REQUIREMENTS.md` and paste the output there.

### Step 2: Architecture Design
1.  Open `prompts/02_architecture_design.md`.
2.  Paste it into the Web LLM (attach `docs/01_REQUIREMENTS.md` as context).
3.  **Review the Output:** Ensure it has C4 Diagrams (Mermaid), API Definitions, and Data Models.
4.  **Save:** Create `docs/02_ARCHITECTURE.md` and paste the output there.

### Step 3: Security & Compliance
1.  Open `prompts/03_security_compliance.md`.
2.  Paste it into the Web LLM (attach `docs/01_REQUIREMENTS.md` and `docs/02_ARCHITECTURE.md`).
3.  **Review the Output:** Ensure it has Threat Models (STRIDE) and AuthZ matrices.
4.  **Save:** Create `docs/03_SECURITY.md` and paste the output there.

### Step 4: Testing Strategy
1.  Open `prompts/04_testing_strategy.md`.
2.  Paste it into the Web LLM (attach previous docs).
3.  **Save:** Create `docs/04_TESTING.md`.

### Step 5: Implementation Plan
1.  Open `prompts/05_implementation_plan.md`.
2.  Paste it into the Web LLM (attach all 4 docs).
3.  **Save:** Create `docs/05_IMPLEMENTATION.md`.
4.  **Action:** This file will contain a `task.md` section. Copy that into your IDE's `task.md` to start coding!

---

## Phase 2: The Build Loop

**Goal:** Implement the spec using the Antigravity Agent (in your IDE).

1.  **Initialize:**
    *   Ensure `task.md` is populated from Step 5.
    *   Ensure `docs/` folder contains all 5 artifacts.

2.  **Execute:**
    *   Point your IDE Agent to the `task.md`.
    *   The Agent will read the `docs/` to understand *exactly* what to build.
    *   Iterate through the tasks until completion.

---

## Pro Tips
*   **Iterate:** If the LLM output isn't detailed enough, ask it to "Critique and Refine" before saving.
*   **Diagrams:** Use the Mermaid diagrams in `02_ARCHITECTURE.md` to visualize the system. You can render them in GitHub or your IDE.
*   **Context:** Always provide the previous documents as context for the next step. This ensures consistency.

```

## File: prompts/00_MASTER_PROTOCOL_LEAN.md
```md
# Quantum Diamond Forge - Master Protocol (Lean Mode)

## Before You Start: Choose Your Mode

**You are viewing: Lean Mode** (~10 artifacts, ~1 hour)

**Other modes available:**
- **Ultra-Lean Mode**: 5 artifacts, ~30 min ‚Üí `prompts/00_MASTER_PROTOCOL_ULTRA_LEAN.md`
- **Enterprise Mode**: 25+ artifacts, 4-6 hours ‚Üí `prompts/00_MASTER_PROTOCOL.md`

**Use Lean Mode if:**
- Building an MVP
- Solo developer or small team
- Need some rigor but not full enterprise
- Time budget: ~1 hour for specs

**Consider a different mode if:**
- Need faster (30 min) ‚Üí Ultra-Lean
- Need full compliance ‚Üí Enterprise

---

## Philosophyion**: 2.1 Lean  
**Audience**: Solo developers, MVPs, prototypes, small teams  
**Philosophy**: Speed + Essential Rigor

---

## What is Lean Mode?

Lean Mode is a **streamlined variant** of Protocol v2.0 designed for:
- Solo developers building MVPs
- Prototypes and proof-of-concepts
- Small teams moving fast
- Projects that don't require full enterprise compliance

**What You Skip:**
- C4 Level 2-3 diagrams (keep Level 1 only)
- Detailed threat modeling tables (basic security checklist instead)
- Extensive non-functional requirements
- Full test pyramid (focus on unit + integration)

**What You Keep:**
- Core requirements (feature catalog, user stories)
- Basic architecture (C4 Level 1, API design, data models)
- Essential security (AuthN/AuthZ, encryption)
- Practical testing (unit + integration tests)
- Implementation plan

---

## The Lean Workflow

```mermaid
graph TD
    User["üë§ You"] -->|1. Idea| WebLLM["üß† Web LLM"]
    WebLLM -->|2. Lean Specs| Specs["üìÑ Essential Docs"]
    Specs -->|3. Guides| Agent["ü§ñ IDE Agent"]
    Agent -->|4. Builds| Code["üíª MVP Code"]
```

### Phase 1: Specification Loop (Lean)

#### Step 1: Requirements (Lean)
**Prompt**: `prompts/lean/01_requirements_analysis.md`

**Output**: `docs/01_REQUIREMENTS.md`
- Feature catalog (Must-Have + Should-Have only)
- 3-5 key user stories
- Basic acceptance criteria

**Time**: ~10 minutes

---

#### Step 2: Architecture (Lean)
**Prompt**: `prompts/lean/02_architecture_design.md`

**Output**: `docs/02_ARCHITECTURE.md`
- C4 Level 1 (System Context only)
- API endpoint list (no full OpenAPI spec)
- Data models (tables + key relationships)

**Time**: ~15 minutes

---

#### Step 3: Security (Lean)
**Prompt**: Use standard `prompts/03_security_compliance.md` but focus on:
- AuthN/AuthZ strategy
- Encryption at rest/transit
- Input validation
- Skip: Full STRIDE, compliance mapping

**Output**: `docs/03_SECURITY.md` (simplified)

**Time**: ~10 minutes

---

#### Step 4: Testing (Lean)
**Prompt**: `prompts/lean/04_testing_strategy.md`

**Output**: `docs/04_TESTING.md`
- Unit test strategy
- Integration test strategy
- Skip: E2E, performance, security testing

**Time**: ~10 minutes

---

#### Step 5: Implementation Plan (Lean)
**Prompt**: `prompts/lean/05_implementation_plan.md`

**Output**: `docs/05_IMPLEMENTATION.md`
- Task breakdown (focus on MVP features)
- Dependency order
- Skip: Detailed ADRs, rollback plans

**Time**: ~10 minutes

---

### Phase 2: Build Loop

Use the standard Antigravity agent workflow:
1. Point agent to `docs/05_IMPLEMENTATION.md`
2. Agent reads specs and builds features
3. Agent runs tests
4. Iterate

---

## When to Upgrade to Enterprise Mode

Switch to full Protocol v2.0 when:
- You're raising funding
- You need SOC2/HIPAA compliance
- Team grows beyond 5 people
- You're handling sensitive data (PII, PHI, financial)
- You need detailed audit trails

---

## Escape Hatches

### "I just want to prototype"
Skip Steps 3-4 entirely. Just do:
1. Requirements (Step 1)
2. Architecture (Step 2)
3. Build

### "I need to iterate mid-build"
- Update the relevant `docs/*.md` file
- Tell the agent: "Requirements changed, re-read docs/01_REQUIREMENTS.md"
- Continue building

### "This is taking too long"
- Use verbosity controls in prompts (select "Lightweight" when asked)
- Skip optional sections
- Focus on Must-Have features only

---

## Lean Mode Checklist

- [ ] Read this guide
- [ ] Run Step 1 (Requirements)
- [ ] Run Step 2 (Architecture)
- [ ] Run Step 3 (Security) - optional but recommended
- [ ] Run Step 4 (Testing) - optional but recommended
- [ ] Run Step 5 (Implementation Plan)
- [ ] Point IDE agent to `docs/05_IMPLEMENTATION.md`
- [ ] Build!

---

## Comparison: Lean vs Enterprise

| Aspect | Lean Mode | Enterprise Mode |
|--------|-----------|-----------------|
| **Artifacts** | ~10 docs | 25+ docs |
| **Time to Spec** | ~1 hour | ~4-6 hours |
| **C4 Diagrams** | Level 1 only | Levels 1-3 |
| **Threat Modeling** | Checklist | Full STRIDE tables |
| **Testing** | Unit + Integration | Full pyramid + perf + security |
| **Compliance** | None | SOC2, HIPAA, GDPR |
| **Best For** | MVPs, prototypes | Production, enterprises |

---

## Next Steps

1. Copy `prompts/lean/01_requirements_analysis.md` to your Web LLM
2. Paste your idea
3. Save the output to `docs/01_REQUIREMENTS.md`
4. Repeat for Steps 2-5
5. Start building!

**Need full rigor?** Switch to `prompts/00_MASTER_PROTOCOL.md` (Enterprise Mode)

```

## File: prompts/00_MASTER_PROTOCOL_ULTRA_LEAN.md
```md
# Quantum Diamond Forge - Ultra-Lean Mode

## Before You Start: Choose Your Mode

**You are viewing: Ultra-Lean Mode** (5 artifacts, ~30 min)

**Other modes available:**
- **Lean Mode**: ~10 artifacts, ~1 hour ‚Üí `prompts/00_MASTER_PROTOCOL_LEAN.md`
- **Enterprise Mode**: 25+ artifacts, 4-6 hours ‚Üí `prompts/00_MASTER_PROTOCOL.md`

**Use Ultra-Lean Mode if:**
- Weekend project or hackathon
- Rapid prototype
- Time budget: ~30 minutes
- You know the domain well

**Consider a different mode if:**
- Need more structure ‚Üí Lean
- Need full rigor ‚Üí Enterprise

---

## Philosophy
**"Ship it this weekend."**

Ultra-Lean mode is for rapid prototypes, hackathons, and quick experiments where speed matters more than completeness. You'll generate just enough spec to start building‚Äîeverything else gets inferred by the IDE agent.

## When to Use Ultra-Lean

‚úÖ **Use Ultra-Lean when:**
- Weekend projects or hackathons
- Quick proof-of-concepts
- Testing an idea before committing
- Solo developer, tight deadline
- You know the domain well

‚ùå **Don't use Ultra-Lean when:**
- Production applications
- Team projects (use Lean or Enterprise)
- Compliance requirements
- Complex integrations
- You need detailed documentation

## The Ultra-Lean Workflow

**Total Time**: ~30 minutes  
**Total Artifacts**: 5

### Step 1: Product Overview (5 min)
**Prompt**: `prompts/ultra-lean/00_overview.md`  
**Output**: `docs/00_OVERVIEW.md`

Defines: Name, tagline, problem, solution, target users

### Step 2: Core Features (5 min)
**Prompt**: `prompts/ultra-lean/01_features.md`  
**Output**: `docs/01_FEATURES.md`

Defines: 3-5 Must-Have features with minimal acceptance criteria

### Step 3: System Diagram (5 min)
**Prompt**: `prompts/ultra-lean/02_system.md`  
**Output**: `docs/02_SYSTEM.md`

Defines: C4 Level 1 diagram + suggested tech stack

### Step 4: API Sketch (10 min)
**Prompt**: `prompts/ultra-lean/03_api.md`  
**Output**: `docs/03_API.md`

Defines: 5-10 endpoints + inferred data models

### Step 5: Build Plan (5 min)
**Prompt**: `prompts/ultra-lean/04_plan.md`  
**Output**: `docs/04_PLAN.md`

Defines: 5-10 implementation tasks with time estimates

## What Gets Auto-Generated

The IDE agent will infer and generate:
- Detailed data models (from API endpoints)
- Test cases (basic templates)
- Security checklist (standard template)
- Error handling patterns
- Validation rules

## Mode Comparison

| Mode | Artifacts | Time | Best For |
|------|-----------|------|----------|
| **Ultra-Lean** | 5 | ~30 min | Rapid prototypes, hackathons |
| **Lean** | ~10 | ~1 hour | MVPs, solo devs |
| **Enterprise** | 25+ | 4-6 hours | Production apps, teams |

## Example: QuickPoll

**Idea**: "Simple polling app for teams"

**Ultra-Lean Spec** (30 minutes):
1. Overview: Problem, solution, users
2. Features: Create poll, vote, view results, share link, export CSV
3. System: Web app ‚Üí API ‚Üí SQLite
4. API: 7 endpoints (polls CRUD, vote, results, export)
5. Plan: 8 tasks, 21 hours over 3 days

**Result**: Enough to start building immediately.

## Tips for Success

1. **Be ruthless**: Only include what you absolutely need
2. **Trust the agent**: Let it infer details from context
3. **Iterate fast**: Build first, refine later
4. **Keep it simple**: Boring tech wins for prototypes
5. **Time-box it**: If a step takes >10 min, you're overthinking

## Upgrading Later

You can always upgrade from Ultra-Lean to Lean or Enterprise:
- Add more features ‚Üí Lean mode
- Add security/compliance ‚Üí Enterprise mode
- Add detailed architecture ‚Üí Enterprise mode

## Ready?

Start with `prompts/ultra-lean/00_overview.md` and work through the 5 steps sequentially.

```

## File: prompts/00_interactive_kickoff.md
```md
# Interactive Kickoff: The Antigravity Guide

**Act as**: The "Antigravity Guide," an expert software architect and product strategist.

**Your Goal**: Interview the user to understand their project, recommend the right "Flight Path" (Mode), and generate a tailored roadmap.

---

## Phase 1: The Interview

Start by greeting the user and asking these 3 questions (one by one or together, your choice):

1.  **The Idea**: "In 1-2 sentences, what are you building?"
2.  **The Goal**: "Is this a weekend prototype, an MVP for users, or a production enterprise app?"
3.  **The Constraints**: "What is your timeline? (e.g., 2 days, 2 weeks, 2 months)"

**Wait for the user's response.**

---

## Phase 2: The Recommendation

Based on their answers, recommend one of the following modes:

### ‚ö° Option A: Ultra-Lean Mode
*   **Best for**: Weekend projects, hackathons, rapid prototypes.
*   **Timeline**: ~30 minutes to spec.
*   **Artifacts**: 5 minimal docs.
*   **Why**: "You want to ship fast and don't need heavy documentation yet."

### üèÉ Option B: Lean Mode
*   **Best for**: MVPs, solo founders, small teams.
*   **Timeline**: ~1 hour to spec.
*   **Artifacts**: ~10 docs (User Stories, Basic Security).
*   **Why**: "You need a solid plan but can't afford enterprise bureaucracy."

### üè¢ Option C: Enterprise Mode
*   **Best for**: Production apps, large teams, compliance.
*   **Timeline**: 4-6 hours to spec.
*   **Artifacts**: 25+ docs (C4 Level 3, STRIDE, Compliance).
*   **Why**: "You need rigorous security and scalability from Day 1."

**Ask the user to confirm their choice.**

---

## Phase 3: The Roadmap

Once they choose a mode, generate a **Step-by-Step Roadmap** for them.

**If Ultra-Lean:**
1.  **Product Overview**: Run `prompts/ultra-lean/00_overview.md`
2.  **Core Features**: Run `prompts/ultra-lean/01_features.md`
3.  **System Diagram**: Run `prompts/ultra-lean/02_system.md`
4.  **API Sketch**: Run `prompts/ultra-lean/03_api.md`
5.  **Build Plan**: Run `prompts/ultra-lean/04_plan.md`
6.  **Execution**: Hand artifacts to IDE Agent.

**If Lean:**
1.  **Requirements**: Run `prompts/lean/01_requirements_analysis.md`
2.  **Architecture**: Run `prompts/lean/02_architecture_design.md`
3.  **Testing**: Run `prompts/lean/04_testing_strategy.md`
4.  **Plan**: Run `prompts/lean/05_implementation_plan.md`
5.  **Lockfiles**: Run `./forge.sh lock all`
6.  **Execution**: Hand artifacts to IDE Agent.

**If Enterprise:**
1.  **Requirements**: Run `prompts/01_requirements_analysis.md`
2.  **Architecture**: Run `prompts/02_architecture_design.md`
3.  **Security**: Run `prompts/03_security_compliance.md`
4.  **Testing**: Run `prompts/04_testing_strategy.md`
5.  **Plan**: Run `prompts/05_implementation_plan.md`
6.  **Lockfiles**: Run `./forge.sh lock all`
7.  **Execution**: Hand artifacts to IDE Agent.

---

## Final Output

End with this message:
> "Ready to start? Copy the prompt for **Step 1** of your chosen mode and paste it here. I'm ready when you are."

```

## File: prompts/01_requirements_analysis.md
```md
Act as a Principal Product Manager operating under the "Antigravity" framework.

**Input:**
I have a rough idea for an application:
[INSERT YOUR IDEA HERE]

**Verbosity Level** (choose one):
- **Full Detail**: Complete enterprise specification with all sections
- **Medium Detail**: Core sections with abbreviated examples
- **Lightweight**: Essential features and requirements only

**Task:**
Analyze this idea and output a structured **Requirements Analysis** in Markdown.
Your goal is to produce a specification matching the requested verbosity level.

**Output Format (01_REQUIREMENTS.md):**

# Requirements Analysis: [App Name]

## 1. Executive Summary
*   **Elevator Pitch**: One sentence value proposition.
*   **The Problem**: The specific friction or pain point being solved.
*   **The Solution**: The functional and emotional resolution.
*   **Target Audience**: Primary and secondary personas.

## 2. Feature Catalog (MoSCoW)
Group features by "Must Have", "Should Have", "Could Have", and "Won't Have".

### Must Have (MVP)
*   **[F-001] Feature Name**:
    *   **Description**: Detailed description of functionality.
    *   **User Benefit**: Why this matters to the user.
    *   **Acceptance Criteria**: Bullet points of what "done" looks like.

### Should Have (v1.1)
*   ...

## 3. Functional Requirements
For each "Must Have" feature, define the technical requirements:

| ID | Requirement | Input | Output | Validation Rule |
|----|-------------|-------|--------|-----------------|
| F-001-RQ-01 | System shall... | User clicks... | System displays... | Email must be unique |

## 4. Non-Functional Requirements
*   **Performance**: (e.g., <100ms API response, support 10k concurrent users).
*   **Security**: (e.g., MFA required, Data encrypted at rest).
*   **Compliance**: (e.g., GDPR, SOC2, HIPAA).
*   **Reliability**: (e.g., 99.9% uptime SLA).

## 5. User Stories
*   **US-001**: As a [Persona], I want to [Action], so that [Benefit].
    *   **Acceptance Criteria**: ...

```

## File: prompts/02_architecture_design.md
```md
Act as a Principal Software Architect.

**Input:**
I have provided the `01_REQUIREMENTS.md` for a new application.

**Task:**
Design the Technical Architecture for this product. Output a **Technical Blueprint** in Markdown.
Focus on scalability, security, and maintainability.

**Output Format (02_ARCHITECTURE.md):**

# Architecture Design

## 1. System Context (C4 Level 1)
*   **Diagram**: Mermaid diagram showing the system and its external dependencies (Users, External APIs, Legacy Systems).
*   **Description**: Narrative of the high-level interactions.

## 2. Container Architecture (C4 Level 2)
*   **Diagram**: Mermaid diagram showing the internal containers (Web App, API, Database, Worker, Cache).
*   **Technology Choices**:
    *   **Frontend**: (e.g., Next.js, React, Tailwind).
    *   **Backend**: (e.g., Node.js/NestJS, Python/FastAPI).
    *   **Database**: (e.g., PostgreSQL, MongoDB).
    *   **Infrastructure**: (e.g., AWS, Vercel, Docker).

## 3. Data Model
*   **ER Diagram**: Mermaid `erDiagram` showing entities and relationships.
*   **Schema Definition**: Brief description of key tables/collections and their purpose.

## 4. API Design
*   **Interface Definition**: High-level description of key API endpoints (REST or GraphQL).
    *   `GET /api/v1/resource`: Description.
    *   `POST /api/v1/resource`: Description.
*   **Integration Patterns**: How do services communicate? (Sync HTTP, Async Events/Kafka).

## 5. Cross-Cutting Concerns
*   **Observability**: Logging, Metrics, Tracing strategy.
*   **Error Handling**: Standardized error response format.
*   **Caching**: Strategy for read-heavy data (Redis, CDN).

```

## File: prompts/03_security_compliance.md
```md
Act as a Chief Information Security Officer (CISO).

**Input:**
I have provided the `01_REQUIREMENTS.md` and `02_ARCHITECTURE.md`.

**Task:**
Develop a comprehensive **Security & Compliance Strategy**.
Do not be generic. Be specific to the architecture and requirements defined.

**Output Format (03_SECURITY.md):**

# Security & Compliance Strategy

## 1. Threat Modeling (STRIDE)
Analyze the architecture for threats:
*   **Spoofing**: How do we prevent impersonation? (AuthN strategy).
*   **Tampering**: How do we ensure data integrity? (Signatures, Checksums).
*   **Repudiation**: How do we prove actions happened? (Audit Logs).
*   **Information Disclosure**: How do we prevent leaks? (Encryption, PII handling).
*   **Denial of Service**: How do we ensure availability? (Rate limiting, WAF).
*   **Elevation of Privilege**: How do we enforce permissions? (RBAC/ABAC).

## 2. Authentication & Authorization
*   **AuthN**: Identity Provider (Auth0, Cognito, Supabase Auth), MFA policy.
*   **AuthZ**: Role definitions, Permission matrix, Policy enforcement points.

## 3. Data Protection
*   **Data Classification**: Public, Internal, Confidential, Restricted.
*   **Encryption**:
    *   **At Rest**: Database encryption, Key management (KMS).
    *   **In Transit**: TLS 1.3, mTLS for internal services.

## 4. Compliance Controls
Map requirements to relevant standards (if applicable):
*   **SOC2**: Access controls, Change management.
*   **GDPR/CCPA**: Right to be forgotten, Data portability.
*   **HIPAA**: PHI handling (if healthcare related).

## 5. Secure Development Lifecycle (SDLC)
*   **Pre-commit**: Secret scanning, SAST (Static Analysis).
*   **CI/CD**: DAST (Dynamic Analysis), Dependency scanning (SCA).

```

## File: prompts/04_testing_strategy.md
```md
Act as a QA Lead / Test Architect.

**Input:**
I have provided the `01_REQUIREMENTS.md` and `02_ARCHITECTURE.md`.

**Task:**
Design a **Master Test Strategy** to ensure the system meets all functional and non-functional requirements.

**Output Format (04_TESTING.md):**

# Master Test Strategy

## 1. Test Pyramid Strategy
Define the scope and tools for each level:
*   **Unit Tests**: (Jest/Vitest) - Coverage target (e.g., 80%), mocking strategy.
*   **Integration Tests**: (Supertest/Testcontainers) - API contract verification, database interaction.
*   **E2E Tests**: (Playwright/Cypress) - Critical user journeys to automate.

## 2. Performance Testing
*   **Load Testing**: (k6/Gatling) - Scenarios to simulate (e.g., Black Friday traffic).
*   **Benchmarks**: Latency targets for key endpoints.

## 3. Security Testing
*   **SAST**: Tools to run in CI (e.g., SonarQube, CodeQL).
*   **DAST**: Automated vulnerability scanning (e.g., OWASP ZAP).

## 4. User Acceptance Testing (UAT)
*   **Plan**: How will stakeholders verify features?
*   **Beta Testing**: Strategy for phased rollout.

## 5. CI/CD Quality Gates
*   Define the conditions that must be met to merge code or deploy to production.
    *   e.g., "All tests pass", "Coverage > 80%", "No critical vulnerabilities".

```

## File: prompts/05_implementation_plan.md
```md
Act as a Technical Project Manager and Lead Engineer.

**Input:**
I have provided the full documentation suite:
1.  `01_REQUIREMENTS.md`
2.  `02_ARCHITECTURE.md`
3.  `03_SECURITY.md`
4.  `04_TESTING.md`

**Task:**
Synthesize these documents into a concrete **Implementation Plan** for an AI Agent (like Antigravity).
Break down the work into sequential, verifiable tasks.

**Output Format (05_IMPLEMENTATION.md):**

# Implementation Plan

## 1. Project Setup
*   [ ] Initialize repository with standard tooling (Linter, Formatter, Husky).
*   [ ] Configure CI/CD pipelines (GitHub Actions).
*   [ ] Set up infrastructure (Terraform/Docker).

## 2. Core Foundation
*   [ ] Implement Database Schema & Migrations.
*   [ ] Implement Authentication & Authorization module.
*   [ ] Implement Base API Structure (Health checks, Logging).

## 3. Feature Implementation (Iterative)
Group by Feature ID from Requirements:
*   **Feature [F-001]**:
    *   [ ] Backend: API endpoints.
    *   [ ] Frontend: UI Components.
    *   [ ] Tests: Unit & Integration.

## 4. Verification & Launch
*   [ ] Run full regression suite.
*   [ ] Perform security audit.
*   [ ] Deploy to Staging.
*   [ ] Smoke test Staging.
*   [ ] Promote to Production.

## 5. Agent Instructions (`task.md`)
Generate the initial `task.md` file content that the AI Agent will use to track progress.

```

## File: prompts/REDTEAM/gpt5-response.md
```md
Below is your **complete Red-Team Review of Quantum Diamond Forge v2.1**, grounded directly in the v2.0 snapshot and the changes introduced in v2.1.
Citations refer to the contents of the uploaded snapshot files.

---

# **Quantum Diamond Forge v2.1 ‚Äî Red-Team Analysis**

## **Executive Summary**

QDF v2.1 is a **major improvement** over v2.0, successfully reducing cognitive load through a Lean mode, modular prompts, and fewer required artifacts. However, the protocol **still carries significant complexity**, relies heavily on human ceremony, and risks becoming a ‚Äúdocumentation machine‚Äù instead of a velocity amplifier. The Enterprise track is strong but still brittle, and Lean mode is helpful but not yet frictionless.

Overall: **Massively improved; not yet optimized for real-world adoption.**

---

# **Strengths**

### **1. Lean Mode is a real breakthrough**

The split into Lean vs. Enterprise tracks directly addresses the ‚Äúprotocol bloat‚Äù highlighted in the v2.0 red-team audits. Developers who would have abandoned v2.0 by Phase 2 now have a viable ~1 hr path.

### **2. Modular prompt architecture dramatically increases reliability**

Breaking monolithic prompts into modules (e.g., `feature_catalog.md`, `c4_diagrams.md`, `threat_modeling.md`) fixes the prompt brittleness issue cited in the v2.0 audit. LLMs handle shorter prompts far more reliably.

### **3. Better Developer Experience via reduced artifact count**

The Lean workflow‚Äôs reduction from 25+ artifacts to ~10 addresses one of the largest adoption barriers. This is backed directly by the v2.0 red-team findings that developers would not maintain the original artifact set.

### **4. Orchestration model remains a differentiator**

Human ‚Üí Web LLM ‚Üí IDE Agent is still the strongest part of the entire protocol. The role separation is clear and matches real-world AI development patterns.

### **5. Documentation quality is high**

The new `GUIDE.md` and modular prompts are more structured, clearer, and easier to follow than the v2.0 version. The Inbox workflow also remains a powerful utility.

---

# **Weaknesses**

### **1. Lean mode is ‚Äúlighter‚Äù but not actually *lean***

Lean mode still produces:

* Feature catalog
* User stories
* C4 Level 1
* Data model
* Basic security
* Testing plan
* Implementation plan

For a solo developer or a fast iteration cycle, this is still ceremony.
Lean mode currently feels like ‚ÄúEnterprise-lite,‚Äù not truly lean.

### **2. No automated *sync* between artifacts**

A major v2.0 flaw remains unresolved:
**spec drift** between requirements ‚Üí architecture ‚Üí API ‚Üí data models ‚Üí tests.
Even with fewer documents, the drift problem persists because:

The protocol offers *structure* but no *synchronization*.

### **3. Too much manual copy/paste and switching between tools**

Most steps still require:

* Open prompt
* Copy prompt
* Paste into LLM
* Copy output
* Save file
* Repeat

This choreography is still a major adoption barrier.

### **4. Human-in-the-loop burden is still heavy**

Even in Lean mode, the human must:

* Review multiple artifacts
* Detect inconsistencies
* Guide iterations
* Reconcile conflicts between LLM outputs

This friction is a large threat to daily use.

### **5. Lean mode still assumes stable context in the Web LLM**

But using *multiple modular prompts* without cross-validation means:
**LLMs will contradict earlier artifacts unless explicitly reminded**.

This is exactly the context-compression problem found in the v2.0 analysis.

### **6. Enterprise mode remains over-optimized for completeness**

While modular prompts help, the overall workflow is still complex and may still be too heavy for teams not explicitly pursuing regulatory-grade rigor.

### **7. No clear recovery path when artifacts disagree**

Example:
If API spec contradicts data model, who resolves it?
Human? IDE Agent? Web LLM?

---

# **Recommendations (Actionable)**

## **1. Introduce a ‚ÄúLockfile System‚Äù (Critical)**

This was a top recommendation in the v2.0 red-team content.
QDF v2.1 still needs:

```
requirements.lock.json
architecture.lock.json
api.lock.json
security.lock.json
testing.lock.json
```

The IDE agent should **only** consume lockfiles.
Each Web LLM operation updates or validates the lockfiles.
This fully solves drift.

---

## **2. Create a ‚ÄúSpec Validator‚Äù CLI step**

Before the IDE Agent runs:

```
qdf validate
```

Should check:

* APIs referenced in C4 exist in API spec
* Data models match API
* Requirements map to tests
* Security model references actual endpoints

This reduces cognitive burden on the human.

---

## **3. Make Lean mode *truly lean***

Current Lean mode still requires too many artifacts.
A more realistic Lean mode:

**Artifacts:**

1. Product overview
2. 3‚Äì5 must-have features
3. One diagram (system context)
4. Minimal API list
5. Implementation plan

Everything else auto-generated by the IDE Agent on demand.

---

## **4. Add an ‚ÄúIteration Loop‚Äù for changing requirements**

You need explicit protocols like:

* `QDF revise requirements`
* `QDF revise architecture`
* `QDF revise api`

These generate:

* diffs
* lockfile updates
* breaking changes lists

This keeps the protocol resilient over time.

---

## **5. Reduce prompt ceremony with a CLI wrapper**

Example:

```
qdf spec requirements
qdf spec architecture
qdf spec security --lean
qdf build
```

CLI generates prompts and sends them to the Web LLM automatically.

This removes 70% of the manual workflow friction.

---

## **6. Introduce ‚ÄúHuman Effort Controls‚Äù**

Each module prompt should ask:

> Do you want:
> (a) Full enterprise detail
> (b) Medium detail
> (c) Lightweight summary

Let the user modulate verbosity.

---

## **7. Automate artifact generation wherever possible**

For example:

* Basic security checklist can be templated
* Test pyramid skeleton can be auto-populated
* Simple C4 Level 1 diagram can use a reusable template

Automation should remove the burden from the human.

---

# **Risk Assessment**

### **1. The biggest threat: Developer Abandonment**

Even with Lean mode, the workflow might still feel burdensome unless automated further.

### **2. Spec Drift**

Without lockfiles + validator, drift will break the protocol during real use.

### **3. Prompt Misalignment Across Modules**

Even modular prompts can conflict unless diffs and validations exist.

### **4. Human Cognitive Overload**

Humans still do too much:

* Reviewing
* Copying
* Reconciling
* Deciding

This doesn‚Äôt scale well.

### **5. IDE Agent Constraints**

If the IDE agent can‚Äôt enforce or understand the spec consistently, build quality drops.

### **6. Over-optimization toward Rigor**

The Enterprise track may still scare away teams unless Lean mode is dramatically leaner.

---

# **Final Verdict**

**QDF v2.1 is a meaningful evolution**, addressing the biggest v2.0 criticisms.
But the protocol still needs:

* Lockfiles
* Validators
* Automated spec generation
* A truly minimalist Lean path
* Better iteration support
* CLI orchestration

With these additions, QDF could evolve from ‚Äúa powerful but heavy framework‚Äù into **the first truly practical AI-native software development protocol**.

If you'd like, I can now generate:

‚úÖ v2.2 Proposal
‚úÖ Lockfile System
‚úÖ Validator Design
‚úÖ Ultra-Lean Mode spec
‚úÖ CLI design for QDF

Just tell me what you want next.

```

## File: prompts/REDTEAM/grok4-response.md
```md
### Executive Summary
Quantum Diamond Forge v2.1 is a commendable pivot toward flexibility with its new Lean mode and modular prompts, addressing v2.0's over-engineering by reducing artifact bloat and enabling faster MVPs for solo devs, while retaining enterprise rigor for larger projects. However, it still clings to a fragmented, copy-paste-heavy workflow that feels archaic in an era of AI agents, risking continued friction and low adoption despite the improvements. The modular approach helps usability but introduces new inconsistencies, and blind spots like inadequate handling of AI variability persist, making it solid but not revolutionary.

### Strengths
- **Dual-Track Workflow Balances Needs:** The addition of Lean mode smartly caters to MVPs with lighter artifacts (e.g., Must-Have features only, no full STRIDE), aligning with pragmatic trends like low-code and AI-assisted prototyping from 2025 software development insights. Enterprise mode maintains depth with C4 diagrams and threat modeling, closely matching AWS Well-Architected pillars (security, reliability) and Google's framework (operational excellence, cost optimization).
- **Modular Prompts Enhance Clarity and Reusability:** Breaking prompts into focused modules (e.g., feature_catalog.md, threat_modeling.md) reduces verbosity by 60-70%, making them more actionable and less overwhelming. This supports better DX, as seen in examples with clear inputs/outputs and realistic instructions (e.g., 3-5 acceptance criteria per user story).
- **Improved Practicality for Solos:** Artifact reduction to ~10 in Lean mode lowers maintenance burden, and CLI tools like forge.sh for tasks/ADRs promote clean habits without overload. Integration of git hooks and CI/CD guides grounds it in real-world best practices like conventional commits and security scanning.
- **Alignment with Industry Trends:** Incorporates 2025 emphases on AI/ML perspectives (e.g., Google's framework) and security-first approaches, with modules covering testing pyramids and data models that echo DRY/YAGNI principles and DevSecOps.

### Weaknesses
- **Persistent Fragmentation and Manual Drudgery:** Despite mods, the workflow remains a clunky sequence of CLI commands, web LLM pastes, and manual saves, creating friction in human-in-the-loop‚Äîe.g., no auto-chaining of modules or API integrations for LLMs, leading to potential context loss or errors. This ignores critiques from X posts where AI struggles with growing projects, amplifying slowdowns (e.g., developers taking 19% longer with AI per studies).
- **Incomplete Coverage in Lean Mode:** Gaps include skimping on performance optimization and sustainability (absent from both modes, despite AWS/Google pillars), and no edge cases for real-time apps, hybrid environments, or international compliance. Enterprise mode's full STRIDE is thorough but overkill, while Lean's basic security checklist risks superficial mitigations for high-impact threats like hallucinations in AI-generated code.
- **Usability Still Lags for Beginners:** Prompts assume familiarity with tools (Mermaid, Husky), and modularization can confuse without strong sequencing‚Äîe.g., no built-in validation if modules are run out-of-order. The "Human-in-the-Loop" amplifies this, as vague review steps invite bias or oversight, clashing with X critiques of AI's "chatty" nature and poor refactoring.
- **Practicality Hindered by Artifact Management:** Even reduced, artifacts like separate MD/JSON files invite staleness without auto-sync or versioning tools, echoing complaints of spending 90% more time reviewing AI code. Adoption barriers persist: No metrics for protocol efficacy (e.g., time-to-MVP), and assumptions of stable LLM access ignore variability (e.g., model updates breaking prompts).
- **Blind Spots in AI Reliance:** Optimizes for rigor but at the expense of agility‚Äîe.g., no mechanisms for handling AI's documented issues like prototype bias or quality crises (67% developer concerns per X insights). Assumes web LLMs produce consistent "gold standard" outputs without A/B testing or fallback prompts.

### Recommendations
- **Automate Orchestration End-to-End:** Introduce a "forge.sh run-all --mode lean" command that chains modules via LLM APIs (if available) or local scripts, auto-assembling artifacts into a single "Master Spec" file with git versioning. Add module dependencies (e.g., require feature_catalog before user_stories) to prevent out-of-order runs.
- **Fill Completeness Gaps with Essentials:** Expand Lean mode to include basic performance/cost checklists (e.g., "Latency targets <100ms") and sustainability notes (e.g., "Energy-efficient hosting"). Add edge-case modules for common scenarios like mobile-first or multi-cloud, and integrate AI-specific guidance from Google's framework (e.g., eval prompts for hallucination checks).
- **Boost Usability with Onboarding and Feedback Loops:** Bundle beginner templates (e.g., pre-filled prompts for common stacks) and add interactive CLI tutorials. Implement a "Post-Phase Review" module with prompts like "Compare output to best practices; suggest fixes," drawing from X critiques to address quality concerns proactively.
- **Enhance Practicality via Metrics and Maintenance:** Embed tracking in CLI (e.g., log phase times) for self-improvement, and use tools like auto-diff scripts to flag artifact drift. Lower barriers with video demos in README and community benchmarks (e.g., "Lean MVP in 45 mins"). For enterprise, mandate cost/benefit analyses in implementation plans.
- **Mitigate Blind Spots Proactively:** Document assumptions explicitly (e.g., "Requires consistent LLM behavior; test with alternatives"). Add redundancy like prompt versioning and fallback to human overrides for AI failures. Optimize for speed by allowing parallel module execution (e.g., architecture alongside security) and include refactoring prompts to counter AI's "chatty" tendencies.

### Risk Assessment
The biggest threat to adoption is the protocol becoming "shelfware"‚Äîdevelopers ditching it for seamless tools like Cursor AI or Replit due to persistent manual friction, especially if LLM inconsistencies (e.g., varying outputs from Gemini/Claude) lead to unreliable specs and wasted time. What could fail: In Lean mode, superficial security causing breaches in production; in Enterprise, over-rigor stifling iteration, amplifying the 19% slowdown from AI per studies. Overall, it risks optimizing for documentation over delivery, at the expense of the "Rigour at Speed" philosophy, in an AI era where agentic workflows demand more automation to stay relevant.
```

## File: prompts/REDTEAM/redteam_analysis.md
```md
# Red Team Analysis: Quantum Diamond Forge Protocol v2.0

## Your Role
You are an **Independent Protocol Auditor** with expertise in:
- Software development methodologies
- AI-assisted development workflows
- Enterprise software architecture
- Developer experience (DX) design

## Context
The Quantum Diamond Forge (QDF) is a protocol for building production-ready software using AI agents. It recently underwent a major upgrade from v2.0 to v2.1 based on red team feedback.

**What Changed from v2.0 to v2.1:**
- **Two-Track Workflow**: Added "Lean" mode for MVPs (~1 hour) alongside "Enterprise" mode (~4-6 hours)
- **Modular Prompts**: Broke monolithic prompts into focused modules (60-70% shorter)
- **Improved DX**: Reduced artifacts from 25+ to ~10 for Lean mode

**The v2.1 Lean Workflow:**
1. **Requirements Analysis** ‚Üí Feature Catalog (Must-Have only), 3-5 User Stories
2. **Architecture Design** ‚Üí C4 Level 1, Simple API list, Data Models
3. **Security** ‚Üí Basic AuthN/AuthZ, Encryption checklist (no full STRIDE)
4. **Testing Strategy** ‚Üí Unit + Integration only (no E2E/perf)
5. **Implementation Plan** ‚Üí MVP task breakdown

**The v2.1 Enterprise Workflow:**
- Same as v2.0 but now with modular prompts for better reliability

**The Orchestration Model:**
- **Human** provides vision and decisions
- **Web LLM** (Gemini/Claude) generates rigorous specs from prompts
- **IDE Agent** (Antigravity) implements code based on specs

## Your Task
Analyze the attached Protocol v2.0 snapshot and provide a **critical, constructive review**.

### Analysis Framework

#### 1. Completeness
- Are there gaps in the workflow?
- What's missing from the prompts?
- Are there edge cases not covered?

#### 2. Usability
- Is this too complex for solo developers?
- Will the "Human-in-the-Loop" model create friction?
- Are the prompts clear and actionable?

#### 3. Rigor vs. Pragmatism
- Does it match enterprise standards?
- Is it over-engineered for typical projects?
- What's the right balance?

#### 4. Practicality
- Will developers actually use this workflow?
- Are the artifacts too heavy to maintain?
- What are the adoption barriers?

#### 5. Blind Spots
- What assumptions are we making?
- What could go wrong?
- What are we optimizing for at the expense of what?

## Output Format

Please structure your response as:

### Executive Summary
[2-3 sentences: Overall assessment]

### Strengths
[What v2 does well]

### Weaknesses
[Critical gaps and concerns]

### Recommendations
[Specific, actionable improvements]

### Risk Assessment
[What could fail? What's the biggest threat to adoption?]

---

**Be brutally honest.** We want to find flaws before users do.

```

## File: prompts/REDTEAM/synthesis.md
```md
# Red Team Analysis Synthesis

**Date**: 2025-11-22  
**Reviewers**: GPT-5, Grok 4  
**Protocol Versions**: v2.0 ‚Üí v2.1  
**Status**: v2.1 implemented, second round of feedback received

## Executive Summary

### v2.0 ‚Üí v2.1 Progress
**v2.1 successfully addressed the top 2 critical issues from v2.0 red team feedback:**
- ‚úÖ Two-Track Workflow implemented (Lean + Enterprise modes)
- ‚úÖ Modular Prompts created (60-70% shorter, more reliable)

**However, v2.1 feedback reveals persistent and new challenges:**
- ‚ö†Ô∏è Lean mode is "lighter" but not truly lean (~10 artifacts still feels heavy)
- üî¥ **Spec drift remains unsolved** (no lockfile/sync mechanism)
- üî¥ **Manual workflow friction persists** (copy-paste ceremony unchanged)
- üî¥ **Critical Risk**: Developer abandonment still a threat despite improvements

### Consensus Assessment (v2.1)
- ‚úÖ **Major improvement** - Lean mode and modular prompts are breakthroughs
- ‚ö†Ô∏è **Still too complex** - Workflow automation and sync mechanisms missing
- üî¥ **New Priority**: Lockfile system + validator are now critical for v2.2

---

## Common Themes

### üéØ Strengths (Both Agree)
1. **Enterprise-grade artifacts** - Formal requirements, C4 diagrams, threat modeling, test strategy
2. **Human-in-the-loop model** - Mitigates AI hallucinations through review gates
3. **Security-first approach** - STRIDE, AuthN/AuthZ, encryption strategies
4. **Deterministic workflow** - Clear 5-step process reduces drift
5. **Modular design** - Prompts are chainable with clear inputs/outputs

### ‚ùå Weaknesses (Both Agree)

#### 1. **Workflow Too Heavy**
- GPT-5: "5-phase waterfall with 25+ artifacts. Solo developers will bail."
- Grok 4: "Overly bureaucratic... excessive manual steps undermine efficiency."

**Impact**: High abandonment risk for non-enterprise users.

#### 2. **Prompt Bloat & Brittleness**
- GPT-5: "Walls of text... LLM robustness decreases as prompt length increases."
- Grok 4: "Prompts are verbose and prescriptive, limiting creativity."

**Impact**: Hallucinated sections, formatting drift, missing subsections.

#### 3. **No Sync Mechanism**
- GPT-5: "No blueprint lockfiles, incremental diffs, or cross-file validation."
- Grok 4: "Maintaining 5+ MD files... could lead to outdated docs."

**Impact**: Guaranteed spec drift as projects evolve.

#### 4. **Manual Overhead**
- GPT-5: "Tedious copy-paste... too much cognitive overhead."
- Grok 4: "No integrated tooling... saving outputs as separate MD files invites version mismatches."

**Impact**: Developer fatigue, human error.

#### 5. **No Escape Hatches**
- GPT-5: "No instructions for skipping steps, iterating quickly, or partial artifacts."
- Grok 4: "No explicit error recovery if LLM outputs diverge."

**Impact**: Rigid system breaks under real-world pressure.

#### 6. **Security Theater**
- GPT-5: "Threat modeling included, but no tooling, validation, or automated checks."
- Grok 4: "Assumes flawless human judgment without checklists."

**Impact**: Security artifacts exist but aren't enforced.

---

## Top Recommendations (Prioritized)

### üî• Critical (Must-Have for v2.1)

#### 1. **Two-Track Workflow**
**Source**: GPT-5, Grok 4  
**What**: Create "Enterprise" and "Lean" tracks
- **Lean**: Skip C4 level 2-3, threat tables, reduce requirements
- **Enterprise**: Full 5-phase process

**Why**: Prevents solo dev abandonment while preserving rigor for teams.

#### 2. **Modular Prompts**
**Source**: GPT-5, Grok 4  
**What**: Break prompts into `/prompts/modules/` (requirements, architecture, security)
- Each module: short, robust, easier for LLMs
- Reduce prompt length by 30%

**Why**: Reduces hallucinations, improves maintainability.

#### 3. **Spec Lockfile System**
**Source**: GPT-5  
**What**: Introduce `requirements.lock.json`, `architecture.lock.json`
- IDE agents read lockfiles, not raw prompts
- Versioned, diffable, prevents drift

**Why**: Solves the #1 technical debt issue.

### ‚ö†Ô∏è High Priority (Should-Have)

#### 4. **Validation & Diff Tools**
**Source**: GPT-5, Grok 4  
**What**: `./forge.sh validate` command
- Checks: data models match API, threat model references real endpoints
- Lints requirements, validates C4 diagrams

**Why**: Shifts burden from human to automation.

#### 5. **Iteration Support**
**Source**: GPT-5, Grok 4  
**What**: Add `./forge.sh revise <phase>` commands
- Generates diffs, lockfile updates, breaking changes lists
- Supports mid-implementation spec changes

**Why**: Real projects evolve; protocol must support iteration.

#### 6. **CLI Automation**
**Source**: Grok 4  
**What**: `./forge.sh run-phase 1` with API wrappers or clipboard automation
- Auto-saves outputs with git versioning
- Adds "lite mode" flag

**Why**: Reduces manual copy-paste friction.

### üí° Nice-to-Have

#### 7. **Human Effort Controls**
**Source**: GPT-5  
**What**: Prompts ask: "Full detail, medium, or lightweight?"
- Modulates output verbosity

**Why**: Reduces cognitive load.

#### 8. **Starter Templates**
**Source**: Grok 4  
**What**: Pre-built templates for common stacks (Next.js/Supabase, etc.)

**Why**: Lowers adoption barriers.

---

## Risk Assessment

### Biggest Threat (Both Agree)
**"Developer abandonment due to workflow heaviness."**

- GPT-5: "Risks becoming a documentation generator rather than productivity booster."
- Grok 4: "Solo devs might abandon after Phase 2 for simpler tools like Cursor."

### What Could Fail
1. **Inconsistent LLM outputs** ‚Üí Spec drift (amplified by weak human review)
2. **Over-reliance on git hooks** ‚Üí Blocked workflows in fast-iterating teams
3. **Artifact maintenance burden** ‚Üí Outdated docs, "shelfware" protocol

---

## Next Steps (v2.0 ‚Üí v2.1)

**Completed Tasks** (v2.1):
1. ‚úÖ **Task 007**: Design Two-Track Workflow (Lean vs Enterprise) - DONE
2. ‚úÖ **Task 008**: Implement Modular Prompt Architecture - DONE
3. ‚úÖ **Task 004**: Design Spec Lockfile System - DONE
4. ‚úÖ **Task 006**: Create Ultra-Lean Mode - DONE

**Remaining Tasks** (v2.2):
5. **Task 009**: Build Spec Validator CLI Tool - BACKLOG
6. **Task 003**: API-Driven Automation Layer - BACKLOG (optional)

**Priority**: v2.1 addressed the top critical feedback. v2.2 focuses on optional automation.

---

## v2.1 Feedback Analysis

### What v2.1 Fixed ‚úÖ
1. **Two-Track Workflow** - Lean mode reduces time from 4-6 hours ‚Üí ~1 hour
2. **Modular Prompts** - 60-70% shorter, more reliable for LLMs
3. **Reduced Artifacts** - From 25+ ‚Üí ~10 in Lean mode
4. **Better DX** - Clearer prompts, better examples

### Persistent Issues from v2.0 üî¥

#### 1. **Spec Drift (Still Unsolved)**
- GPT-5 v2.1: "No automated sync between artifacts... drift problem persists."
- Grok 4 v2.1: "Incomplete coverage... no edge cases for real-time apps."

**Impact**: Requirements ‚Üí Architecture ‚Üí API ‚Üí Data models will diverge over time.

#### 2. **Manual Workflow Friction (Unchanged)**
- GPT-5 v2.1: "Most steps still require: Open prompt ‚Üí Copy ‚Üí Paste ‚Üí Save ‚Üí Repeat."
- Grok 4 v2.1: "Persistent fragmentation... no auto-chaining of modules."

**Impact**: Developer fatigue, high abandonment risk.

#### 3. **Human Cognitive Overload (Still High)**
- GPT-5 v2.1: "Human must review multiple artifacts, detect inconsistencies, reconcile conflicts."
- Grok 4 v2.1: "Usability still lags... vague review steps invite bias."

**Impact**: Doesn't scale for daily use.

### New Issues Identified in v2.1 ‚ö†Ô∏è

#### 4. **Lean Mode Not Lean Enough**
- GPT-5 v2.1: "Lean mode still produces: Feature catalog, user stories, C4, data model, security, testing, implementation. This is still ceremony."
- Grok 4 v2.1: "Gaps include skimping on performance optimization and sustainability."

**Impact**: Lean mode feels like "Enterprise-lite," not truly lean.

#### 5. **No Recovery Path for Conflicts**
- GPT-5 v2.1: "If API spec contradicts data model, who resolves it? Human? IDE Agent? Web LLM?"
- Grok 4 v2.1: "No mechanisms for handling AI's documented issues like prototype bias."

**Impact**: Breaks down when LLMs produce conflicting outputs.

#### 6. **LLM Context Compression Across Modules**
- GPT-5 v2.1: "Using multiple modular prompts without cross-validation means LLMs will contradict earlier artifacts."
- Grok 4 v2.1: "Modularization can confuse without strong sequencing."

**Impact**: Modules may conflict with each other.

---

## Updated Recommendations (v2.2 Priorities)

### üî• Critical (Blockers for Adoption)

#### 1. **Lockfile System** (HIGHEST PRIORITY)
**Source**: GPT-5 v2.1, Grok 4 v2.1 (both emphasize heavily)  
**What**: Introduce lockfiles for each phase (requirements.lock.json, architecture.lock.json, etc.)
- IDE agent reads ONLY lockfiles
- Each Web LLM operation updates/validates lockfiles
- Versioned, diffable, prevents drift

**Why**: Solves spec drift completely. Both reviewers call this "critical."

#### 2. **Spec Validator CLI**
**Source**: GPT-5 v2.1, Grok 4 v2.1  
**What**: qdf validate command that checks cross-artifact consistency

**Why**: Reduces human cognitive burden, catches conflicts automatically.

#### 3. **Ultra-Lean Mode**
**Source**: GPT-5 v2.1, Grok 4 v2.1  
**What**: Truly minimal workflow with 3-5 artifacts max

**Why**: Current Lean mode (10 artifacts) still feels heavy.

---

## Next Steps (v2.2 Roadmap)

### Immediate Actions (Based on Valid Feedback)
1. ‚úÖ **Create Task**: Design and implement lockfile system
2. ‚úÖ **Create Task**: Build spec validator CLI
3. ‚úÖ **Create Task**: Create ultra-lean mode
4. ‚ö†Ô∏è **Create Task**: Design CLI automation layer
5. ‚ö†Ô∏è **Create Task**: Add iteration support

**Priority**: Start with lockfile system and validator as they address the most critical gaps.

```

## File: prompts/lean/01_requirements_analysis.md
```md
# Requirements Analysis (Lean Mode)

## Your Role
You are a **Product Analyst** helping a solo developer or small team build an MVP. Focus on **essential features** and **clear user stories** without excessive ceremony.

## Context
The user has an idea for a software product. Your job is to help them define:
1. **What** they're building (feature catalog)
2. **Who** it's for (user stories)
3. **How** to validate it (acceptance criteria)

Keep it **lean and actionable**‚Äîthis is an MVP, not an enterprise product.

## Input
The user will provide:
- A brief description of their idea
- Target users (if known)
- Key problems they're solving

## Your Task

Generate a **Requirements Document** with the following structure:

---

### 1. Product Overview
**Format**:
```
## Product Overview

**Name**: [Product Name]
**Tagline**: [One-line description]
**Problem**: [What problem does this solve?]
**Solution**: [How does this product solve it?]
```

Keep this to 3-4 sentences total.

---

### 2. Feature Catalog (MoSCoW - Lean)
**Format**:
```
## Feature Catalog

### Must-Have (MVP Core)
1. [Feature Name]: [1-sentence description]
2. [Feature Name]: [1-sentence description]
...

### Should-Have (Post-MVP)
1. [Feature Name]: [1-sentence description]
2. [Feature Name]: [1-sentence description]
...
```

**Rules**:
- **Must-Have**: 3-7 features max (the absolute minimum for a working product)
- **Should-Have**: 3-5 features (nice-to-haves for v1.1)
- **Skip**: Could-Have and Won't-Have (not needed for MVP)

---

### 3. User Stories (Top 3-5 Only)
**Format**:
```
## User Stories

### US-001: [Story Title]
**As a** [user type]  
**I want** [action]  
**So that** [benefit]

**Acceptance Criteria**:
- [ ] [Specific, testable criterion]
- [ ] [Specific, testable criterion]

---

### US-002: [Story Title]
...
```

**Rules**:
- Write **3-5 stories** covering the Must-Have features
- Each story should have **2-4 acceptance criteria**
- Keep criteria **simple and testable**

---

### 4. Non-Functional Requirements (Essentials Only)
**Format**:
```
## Non-Functional Requirements

### Performance
- [1-2 key metrics, e.g., "Page load < 2s"]

### Security
- [1-2 essentials, e.g., "HTTPS only", "Password hashing"]

### Scalability
- [1 sentence on expected load, e.g., "Support 100 concurrent users"]
```

**Rules**:
- **Skip**: Detailed SLAs, compliance requirements, extensive monitoring
- **Keep**: Basic performance, security, and scalability expectations

---

## Output Format

Provide the complete document in **Markdown** format, ready to save as `docs/01_REQUIREMENTS.md`.

---

## Example Output

```markdown
# Requirements Document (Lean)

## Product Overview

**Name**: TaskFlow  
**Tagline**: Simple task management for solo developers  
**Problem**: Developers need a lightweight way to track tasks without complex project management tools.  
**Solution**: A minimal task board with drag-and-drop, markdown support, and GitHub integration.

## Feature Catalog

### Must-Have (MVP Core)
1. **Task Board**: Kanban-style board with To Do, In Progress, Done columns
2. **Markdown Tasks**: Create tasks with markdown descriptions
3. **Drag & Drop**: Move tasks between columns
4. **GitHub Sync**: Import issues from GitHub repos
5. **Local Storage**: Save tasks in browser (no backend needed for MVP)

### Should-Have (Post-MVP)
1. **Tags & Filters**: Organize tasks by tags
2. **Due Dates**: Set deadlines for tasks
3. **Cloud Sync**: Save tasks to a backend for multi-device access

## User Stories

### US-001: Create a Task
**As a** developer  
**I want** to create a task with a title and markdown description  
**So that** I can track what I need to do

**Acceptance Criteria**:
- [ ] User can click "New Task" button
- [ ] User can enter title and markdown description
- [ ] Task appears in "To Do" column

---

### US-002: Move Tasks Between Columns
**As a** developer  
**I want** to drag tasks between columns  
**So that** I can update task status visually

**Acceptance Criteria**:
- [ ] User can drag a task from one column to another
- [ ] Task position updates immediately
- [ ] Changes persist in local storage

---

### US-003: Import GitHub Issues
**As a** developer  
**I want** to import issues from my GitHub repo  
**So that** I don't have to manually recreate tasks

**Acceptance Criteria**:
- [ ] User can enter GitHub repo URL
- [ ] System fetches open issues via GitHub API
- [ ] Issues appear as tasks in "To Do" column

## Non-Functional Requirements

### Performance
- Page load < 2 seconds
- Drag-and-drop feels instant (< 100ms)

### Security
- HTTPS only
- GitHub OAuth for API access

### Scalability
- Support 500 tasks per board
- Handle 10 concurrent users (for future cloud sync)
```

---

## Important Notes

- **Be concise**: This is Lean mode‚Äîno walls of text
- **Focus on MVP**: Only include features needed for a working v1.0
- **Skip enterprise artifacts**: No detailed compliance, no extensive NFRs
- **Make it actionable**: Every feature should be clear enough to build

---

## Ready?

Paste the user's idea below, and I'll generate the lean requirements document.

```

## File: prompts/lean/02_architecture_design.md
```md
# Architecture Design (Lean Mode)

## Your Role
You are a **Software Architect** helping design an MVP. Focus on **essential architecture** without over-engineering.

## Context
The user has defined their requirements. Your job is to design:
1. **System Context** (C4 Level 1 only)
2. **API Endpoints** (simple list, not full OpenAPI)
3. **Data Models** (tables + key relationships)

Keep it **lean and buildable**‚Äîthis is an MVP, not a microservices architecture.

## Input
Attach the `docs/01_REQUIREMENTS.md` file from the previous step.

## Your Task

Generate an **Architecture Document** with the following structure:

---

### 1. System Context (C4 Level 1)
**Format**:
```
## System Context Diagram

\`\`\`mermaid
C4Context
    title System Context - [Product Name]
    
    Person(user, "User", "End user of the system")
    System(app, "[Product Name]", "Core application")
    System_Ext(external, "[External System]", "Description")
    
    Rel(user, app, "Uses")
    Rel(app, external, "Integrates with")
\`\`\`

**Key Components**:
- **[Product Name]**: [1-sentence description]
- **[External System]**: [1-sentence description]
```

**Rules**:
- Show **only** the main system and key external dependencies
- **Skip**: Container diagrams (C4 Level 2), Component diagrams (C4 Level 3)

---

### 2. Technology Stack
**Format**:
```
## Technology Stack

### Frontend
- **Framework**: [e.g., React, Vue, vanilla JS]
- **Styling**: [e.g., Tailwind, CSS]

### Backend
- **Runtime**: [e.g., Node.js, Python]
- **Framework**: [e.g., Express, FastAPI]
- **Database**: [e.g., PostgreSQL, SQLite]

### Deployment
- **Hosting**: [e.g., Vercel, Railway, local]
```

**Rules**:
- Choose **simple, proven** technologies
- Prefer **managed services** over self-hosted (e.g., Supabase over self-hosted Postgres)
- **Skip**: Detailed infrastructure diagrams, Kubernetes, microservices

---

### 3. API Design (Endpoint List)
**Format**:
```
## API Endpoints

### Tasks
- `GET /api/tasks` - List all tasks
- `POST /api/tasks` - Create a new task
- `PATCH /api/tasks/:id` - Update a task
- `DELETE /api/tasks/:id` - Delete a task

### Users
- `POST /api/auth/login` - User login
- `POST /api/auth/logout` - User logout
```

**Rules**:
- List **only** the core endpoints needed for MVP
- Use **RESTful** conventions
- **Skip**: Full OpenAPI spec, detailed request/response schemas

---

### 4. Data Models
**Format**:
```
## Data Models

### Table: users
| Column | Type | Constraints |
|--------|------|-------------|
| id | UUID | PRIMARY KEY |
| email | VARCHAR(255) | UNIQUE, NOT NULL |
| password_hash | VARCHAR(255) | NOT NULL |
| created_at | TIMESTAMP | DEFAULT NOW() |

### Table: tasks
| Column | Type | Constraints |
|--------|------|-------------|
| id | UUID | PRIMARY KEY |
| user_id | UUID | FOREIGN KEY (users.id) |
| title | VARCHAR(255) | NOT NULL |
| description | TEXT | |
| status | ENUM | 'todo', 'in_progress', 'done' |
| created_at | TIMESTAMP | DEFAULT NOW() |

**Relationships**:
- `tasks.user_id` ‚Üí `users.id` (many-to-one)
```

**Rules**:
- Show **only** the core tables needed for MVP
- Include **primary keys, foreign keys, and essential constraints**
- **Skip**: Detailed indexes, triggers, stored procedures

---

## Output Format

Provide the complete document in **Markdown** format, ready to save as `docs/02_ARCHITECTURE.md`.

---

## Important Notes

- **Be pragmatic**: Choose boring, proven tech over cutting-edge
- **Avoid over-engineering**: No microservices, no Kubernetes, no event sourcing
- **Focus on MVP**: Design for 100 users, not 1 million
- **Keep it simple**: If you can do it with a single database table, do it

---

## Ready?

Paste the requirements document below, and I'll generate the lean architecture.

```

## File: prompts/lean/04_testing_strategy.md
```md
# Testing Strategy (Lean Mode)

## Your Role
You are a **QA Engineer** helping define a practical testing strategy for an MVP. Focus on **essential tests** without excessive coverage.

## Context
The user has defined requirements and architecture. Your job is to define:
1. **Unit Test Strategy** (test core logic)
2. **Integration Test Strategy** (test API endpoints)
3. **Manual Testing Checklist** (for features that are hard to automate)

Keep it **lean and practical**‚Äîthis is an MVP, not a safety-critical system.

## Input
Attach the `docs/01_REQUIREMENTS.md` and `docs/02_ARCHITECTURE.md` files.

## Your Task

Generate a **Testing Strategy Document** with the following structure:

---

### 1. Testing Philosophy
**Format**:
```
## Testing Philosophy

For this MVP, we focus on:
- **Unit Tests**: Core business logic and utilities
- **Integration Tests**: API endpoints and database interactions
- **Manual Testing**: UI flows and edge cases

**What We Skip** (for now):
- E2E tests (too slow for MVP iteration)
- Performance tests (optimize later)
- Security tests (basic security in code review)
```

---

### 2. Unit Test Strategy
**Format**:
```
## Unit Tests

### What to Test
- [ ] [Business logic function 1]
- [ ] [Business logic function 2]
- [ ] [Utility function 1]

### Example Test Cases
**Function**: `validateEmail(email)`
- ‚úÖ Valid email: `test@example.com` ‚Üí `true`
- ‚úÖ Invalid email: `notanemail` ‚Üí `false`
- ‚úÖ Empty string: `""` ‚Üí `false`

**Coverage Goal**: 70% of core logic
```

**Rules**:
- List **5-10** key functions to unit test
- Provide **2-3 example test cases** for the most critical function
- **Skip**: 100% coverage, testing trivial getters/setters

---

### 3. Integration Test Strategy
**Format**:
```
## Integration Tests

### API Endpoints to Test
- [ ] `POST /api/tasks` - Create task
- [ ] `GET /api/tasks` - List tasks
- [ ] `PATCH /api/tasks/:id` - Update task
- [ ] `DELETE /api/tasks/:id` - Delete task

### Example Test Case
**Endpoint**: `POST /api/tasks`
- **Setup**: Create a test user
- **Request**: `{ "title": "Test Task", "description": "..." }`
- **Expected**: 201 Created, task appears in database
- **Teardown**: Delete test user and task

**Coverage Goal**: All core API endpoints
```

**Rules**:
- Test **all** core API endpoints from the architecture doc
- Provide **1 detailed example** test case
- **Skip**: Testing every edge case, load testing

---

### 4. Manual Testing Checklist
**Format**:
```
## Manual Testing Checklist

### Pre-Release Checklist
- [ ] User can sign up and log in
- [ ] User can create a task
- [ ] User can drag task between columns
- [ ] User can delete a task
- [ ] UI looks good on mobile
- [ ] UI looks good on desktop

### Edge Cases to Verify
- [ ] What happens if user creates 100 tasks?
- [ ] What happens if task title is very long?
- [ ] What happens if user loses internet connection?
```

**Rules**:
- List **5-10** critical user flows
- Include **3-5** edge cases to manually verify
- **Skip**: Exhaustive test matrices

---

## Output Format

Provide the complete document in **Markdown** format, ready to save as `docs/04_TESTING.md`.

---

## Important Notes

- **Be practical**: Focus on tests that catch real bugs
- **Avoid perfectionism**: 70% coverage is fine for an MVP
- **Manual testing is OK**: Not everything needs to be automated
- **Iterate**: Add more tests as the product matures

---

## Ready?

Paste the requirements and architecture documents below, and I'll generate the lean testing strategy.

```

## File: prompts/lean/05_implementation_plan.md
```md
# Implementation Plan (Lean Mode)

## Your Role
You are a **Technical Lead** helping break down an MVP into actionable tasks. Focus on **getting to a working v1.0 quickly**.

## Context
The user has defined requirements, architecture, and testing strategy. Your job is to create a **task breakdown** that an AI agent (or developer) can follow to build the MVP.

Keep it **lean and actionable**‚Äîthis is an MVP, not a 6-month project.

## Input
Attach all previous documents:
- `docs/01_REQUIREMENTS.md`
- `docs/02_ARCHITECTURE.md`
- `docs/04_TESTING.md`

## Your Task

Generate an **Implementation Plan** with the following structure:

---

### 1. Implementation Phases
**Format**:
```
## Implementation Phases

### Phase 1: Foundation (Week 1)
- Set up project structure
- Configure database
- Implement authentication

### Phase 2: Core Features (Week 2)
- Build task CRUD API
- Implement task board UI
- Add drag-and-drop

### Phase 3: Polish (Week 3)
- Add tests
- Fix bugs
- Deploy to production
```

**Rules**:
- Break into **3-4 phases** max
- Each phase should be **1-2 weeks** of work
- Focus on **Must-Have** features only

---

### 2. Task Breakdown
**Format**:
```
## Task Breakdown

### Phase 1: Foundation

#### Task 1.1: Project Setup
**Description**: Initialize project with chosen tech stack
**Deliverables**:
- [ ] Create Next.js app
- [ ] Set up Tailwind CSS
- [ ] Configure ESLint and Prettier

**Estimated Time**: 2 hours

---

#### Task 1.2: Database Setup
**Description**: Set up PostgreSQL database and ORM
**Deliverables**:
- [ ] Create Supabase project
- [ ] Define `users` and `tasks` tables
- [ ] Test database connection

**Estimated Time**: 3 hours

---

### Phase 2: Core Features

#### Task 2.1: Task CRUD API
**Description**: Implement API endpoints for task management
**Deliverables**:
- [ ] `POST /api/tasks` - Create task
- [ ] `GET /api/tasks` - List tasks
- [ ] `PATCH /api/tasks/:id` - Update task
- [ ] `DELETE /api/tasks/:id` - Delete task

**Estimated Time**: 6 hours
```

**Rules**:
- Each task should be **2-8 hours** of work
- Include **clear deliverables** (checkboxes)
- Provide **time estimates**
- Order tasks by **dependency** (foundation first)

---

### 3. Dependency Graph
**Format**:
```
## Dependency Graph

\`\`\`mermaid
graph TD
    T1[Task 1.1: Project Setup] --> T2[Task 1.2: Database Setup]
    T2 --> T3[Task 2.1: Task CRUD API]
    T3 --> T4[Task 2.2: Task Board UI]
    T4 --> T5[Task 2.3: Drag & Drop]
\`\`\`
```

**Rules**:
- Show **critical path** dependencies
- **Skip**: Complex dependency matrices

---

### 4. MVP Checklist
**Format**:
```
## MVP Checklist

### Must-Have Before Launch
- [ ] User can sign up and log in
- [ ] User can create, edit, delete tasks
- [ ] User can drag tasks between columns
- [ ] Basic tests pass
- [ ] Deployed to production

### Nice-to-Have (Post-Launch)
- [ ] Tags and filters
- [ ] Due dates
- [ ] Cloud sync
```

---

## Output Format

Provide the complete document in **Markdown** format, ready to save as `docs/05_IMPLEMENTATION.md`.

---

## Important Notes

- **Be realistic**: Don't underestimate time
- **Focus on MVP**: Only include Must-Have features
- **Make it actionable**: Each task should be clear enough to start immediately
- **Order matters**: Dependencies should be obvious

---

## Ready?

Paste all previous documents below, and I'll generate the lean implementation plan.

```

## File: prompts/lean/TEST_SCENARIO.md
```md
# Lean Mode Test Scenario

## Test Idea
**Product**: "QuickPoll" - A simple polling app for teams

**Description**: 
A lightweight polling tool where users can create quick polls, share them with their team, and see real-time results. Think Strawpoll but for internal teams.

**Target Users**: Small teams (5-20 people) who need quick decision-making

**Key Problems**: 
- Email threads for simple yes/no questions are slow
- Existing tools (Slack polls, etc.) lack persistence
- Need something dead simple with no setup

---

## Expected Lean Mode Output

### Step 1: Requirements (~10 min)
**Expected Artifacts**:
- Feature catalog: 5-7 Must-Have features (create poll, vote, view results, share link, basic auth)
- 3-5 user stories
- Basic NFRs (page load < 2s, HTTPS, 100 concurrent users)

**Success Criteria**: 
- ‚úÖ Output is concise (< 2 pages)
- ‚úÖ Focuses only on MVP features
- ‚úÖ No extensive compliance requirements

---

### Step 2: Architecture (~15 min)
**Expected Artifacts**:
- C4 Level 1 diagram (QuickPoll app + external auth provider)
- Simple API endpoint list (5-10 endpoints)
- 2-3 database tables (users, polls, votes)

**Success Criteria**:
- ‚úÖ No C4 Level 2-3 diagrams
- ‚úÖ No detailed OpenAPI spec
- ‚úÖ Simple tech stack (e.g., Next.js + Supabase)

---

### Step 3: Security (~10 min)
**Expected Artifacts**:
- AuthN/AuthZ strategy (OAuth or email/password)
- Basic encryption (HTTPS, password hashing)
- Input validation checklist

**Success Criteria**:
- ‚úÖ No full STRIDE tables
- ‚úÖ No SOC2/HIPAA compliance mapping
- ‚úÖ Practical security only

---

### Step 4: Testing (~10 min)
**Expected Artifacts**:
- Unit test strategy (5-10 key functions)
- Integration test strategy (all API endpoints)
- Manual testing checklist (5-10 flows)

**Success Criteria**:
- ‚úÖ No E2E tests
- ‚úÖ No performance/security testing
- ‚úÖ 70% coverage goal (not 100%)

---

### Step 5: Implementation Plan (~10 min)
**Expected Artifacts**:
- 3 phases (Foundation, Core Features, Polish)
- 10-15 tasks with time estimates
- Simple dependency graph

**Success Criteria**:
- ‚úÖ Tasks are 2-8 hours each
- ‚úÖ Focus on Must-Have features only
- ‚úÖ Total MVP timeline: 2-3 weeks

---

## Validation Checklist

### Prompt Quality
- [ ] Prompts are clear and easy to follow
- [ ] Examples are helpful and relevant
- [ ] Instructions are concise (not walls of text)
- [ ] Output format is well-defined

### Workflow Efficiency
- [ ] Total time to complete all 5 steps: ~1 hour
- [ ] Each step builds on the previous one
- [ ] No redundant information requested
- [ ] Easy to copy-paste between steps

### Output Quality
- [ ] Artifacts are actionable (can start building immediately)
- [ ] No over-engineering (appropriate for MVP)
- [ ] Consistent naming and structure
- [ ] Clear enough for an AI agent to implement

---

## Next Steps After Validation

If test passes:
1. ‚úÖ Lean mode is ready for use
2. Continue with Phase 2 (Modular Prompts)
3. Add GUIDE.md mode selector

If test fails:
1. Identify issues in prompts
2. Refine unclear sections
3. Re-test before proceeding

```

## File: prompts/modules/README.md
```md
# Modular Prompt Architecture Guide

## Overview
Instead of using monolithic prompts (like `01_requirements_analysis.md`), you can compose custom workflows using **prompt modules**.

## Why Modular?
- **Shorter prompts** = fewer LLM hallucinations
- **Easier to maintain** = update one module, not entire prompt
- **Flexible** = mix and match modules for your needs
- **Robust** = smaller prompts are more reliable

## How to Use

### Option 1: Use Pre-Composed Prompts (Recommended)
Just use the existing prompts:
- `prompts/01_requirements_analysis.md` (uses all requirements modules)
- `prompts/02_architecture_design.md` (uses all architecture modules)
- etc.

### Option 2: Compose Your Own
Pick and choose modules based on your needs:

**Example: Lightweight Requirements**
```
Use modules:
- prompts/modules/requirements/product_overview.md
- prompts/modules/requirements/feature_catalog.md
- prompts/modules/requirements/user_stories.md

Skip:
- Functional requirements table
- Non-functional requirements
```

**Example: Security-Focused Architecture**
```
Use modules:
- prompts/modules/architecture/c4_diagrams.md
- prompts/modules/architecture/data_models.md
- prompts/modules/security/threat_modeling.md
- prompts/modules/security/authz.md

Skip:
- API design
- Technology stack
```

## Module Directory Structure

```
prompts/modules/
‚îú‚îÄ‚îÄ requirements/
‚îÇ   ‚îú‚îÄ‚îÄ product_overview.md
‚îÇ   ‚îú‚îÄ‚îÄ feature_catalog.md
‚îÇ   ‚îú‚îÄ‚îÄ functional_requirements.md
‚îÇ   ‚îú‚îÄ‚îÄ nonfunctional_requirements.md
‚îÇ   ‚îî‚îÄ‚îÄ user_stories.md
‚îú‚îÄ‚îÄ architecture/
‚îÇ   ‚îú‚îÄ‚îÄ c4_diagrams.md
‚îÇ   ‚îú‚îÄ‚îÄ technology_stack.md
‚îÇ   ‚îú‚îÄ‚îÄ api_design.md
‚îÇ   ‚îî‚îÄ‚îÄ data_models.md
‚îú‚îÄ‚îÄ security/
‚îÇ   ‚îú‚îÄ‚îÄ threat_modeling.md
‚îÇ   ‚îú‚îÄ‚îÄ authz.md
‚îÇ   ‚îî‚îÄ‚îÄ compliance.md
‚îî‚îÄ‚îÄ testing/
    ‚îú‚îÄ‚îÄ test_pyramid.md
    ‚îú‚îÄ‚îÄ performance_testing.md
    ‚îî‚îÄ‚îÄ security_testing.md
```

## Module Format

Each module is self-contained and follows this structure:

```markdown
# [Module Name]

## Purpose
[What this module generates]

## Input Required
[What context the LLM needs]

## Output Format
[Exact structure to generate]

## Example
[Sample output]
```

## Composing Modules

### Method 1: Sequential (Recommended)
Run modules one at a time, feeding output to the next:

1. Run `product_overview.md` ‚Üí save output
2. Run `feature_catalog.md` + attach previous output ‚Üí save
3. Run `user_stories.md` + attach all previous outputs ‚Üí save

### Method 2: Single Prompt
Combine multiple modules into one prompt:

```
I need you to generate:
1. [Copy content from product_overview.md]
2. [Copy content from feature_catalog.md]
3. [Copy content from user_stories.md]
```

**Warning**: This defeats the purpose of modularity. Only use for 2-3 small modules.

## Benefits by Use Case

### Solo Developer (MVP)
**Use**: 3-5 modules only
- Product overview
- Feature catalog (Must-Have only)
- User stories (top 3)
- C4 Level 1
- Data models

**Skip**: Everything else

### Small Team (Production)
**Use**: 8-12 modules
- All requirements modules
- C4 Level 1-2
- API design
- Data models
- Basic threat modeling
- Test pyramid

**Skip**: Compliance, detailed security

### Enterprise Team
**Use**: All modules (15+)
- Full requirements
- C4 Level 1-3
- Full security (STRIDE, compliance)
- Full testing (pyramid + perf + security)

## Migration Guide

### From Monolithic to Modular

**Before** (Monolithic):
```bash
# Copy entire 01_requirements_analysis.md
# Paste to LLM
# Get 5-page output
```

**After** (Modular):
```bash
# Copy product_overview.md
# Paste to LLM ‚Üí get 1 paragraph

# Copy feature_catalog.md
# Paste to LLM + attach overview ‚Üí get feature list

# Copy user_stories.md
# Paste to LLM + attach features ‚Üí get stories
```

**Result**: Same output, but in 3 focused steps instead of 1 giant prompt.

## Tips

1. **Start small**: Use 2-3 modules first, add more as needed
2. **Feed forward**: Always attach previous outputs as context
3. **Iterate**: If output is wrong, re-run just that module
4. **Combine for speed**: If modules are small (<500 words), combine 2-3

## Next Steps

1. Browse `prompts/modules/` to see available modules
2. Pick modules for your use case
3. Run them sequentially
4. Combine outputs into final document

```

## File: prompts/modules/architecture/c4_diagrams.md
```md
# C4 Diagrams Module

## Purpose
Generate C4 architecture diagrams (System Context, Container, Component).

## Input Required
- Product overview
- Feature catalog
- OR brief product description

## Output Format

```markdown
## C4 Architecture Diagrams

### Level 1: System Context
\`\`\`mermaid
C4Context
    title System Context - [Product Name]
    
    Person(user, "[User Type]", "Description")
    System(app, "[Product Name]", "Core application")
    System_Ext(external, "[External System]", "Description")
    
    Rel(user, app, "Uses")
    Rel(app, external, "Integrates with")
\`\`\`

### Level 2: Container Diagram
\`\`\`mermaid
C4Container
    title Container Diagram - [Product Name]
    
    Person(user, "[User Type]")
    
    Container(web, "Web Application", "React", "User interface")
    Container(api, "API", "Node.js", "Business logic")
    ContainerDb(db, "Database", "PostgreSQL", "Data storage")
    
    Rel(user, web, "Uses", "HTTPS")
    Rel(web, api, "Calls", "REST/JSON")
    Rel(api, db, "Reads/Writes", "SQL")
\`\`\`

### Level 3: Component Diagram (Optional)
[Only for complex systems]
```

## Instructions for LLM

1. **Level 1** (Required): Show the system and its external dependencies
2. **Level 2** (Required): Show internal containers (frontend, backend, database)
3. **Level 3** (Optional): Only if system is complex (5+ containers)

Use Mermaid C4 syntax for all diagrams.

## Example Output

```markdown
## C4 Architecture Diagrams

### Level 1: System Context
\`\`\`mermaid
C4Context
    title System Context - TaskFlow
    
    Person(dev, "Developer", "Solo developer managing tasks")
    System(taskflow, "TaskFlow", "Task management application")
    System_Ext(github, "GitHub API", "Issue tracking")
    
    Rel(dev, taskflow, "Manages tasks")
    Rel(taskflow, github, "Imports issues", "REST API")
\`\`\`

### Level 2: Container Diagram
\`\`\`mermaid
C4Container
    title Container Diagram - TaskFlow
    
    Person(dev, "Developer")
    
    Container(spa, "Single Page App", "React", "Task board UI")
    Container(api, "API Server", "Node.js/Express", "Business logic")
    ContainerDb(db, "Database", "PostgreSQL", "Task storage")
    System_Ext(github, "GitHub API")
    
    Rel(dev, spa, "Uses", "HTTPS")
    Rel(spa, api, "API calls", "REST/JSON")
    Rel(api, db, "Reads/Writes", "SQL")
    Rel(api, github, "Fetches issues", "REST API")
\`\`\`
```

## Ready?

Paste the product overview and feature catalog below, and I'll generate the C4 diagrams.

```

## File: prompts/modules/architecture/data_models.md
```md
# Data Models Module

## Purpose
Generate database schema with tables, columns, and relationships.

## Input Required
- Feature catalog
- User stories
- OR brief product description

## Output Format

```markdown
## Data Models

### Table: [table_name]
| Column | Type | Constraints |
|--------|------|-------------|
| id | UUID | PRIMARY KEY |
| [column] | [type] | [constraints] |
...

**Indexes**:
- `idx_[table]_[column]` on `[column]`

---

### Table: [table_name]
...

## Relationships

\`\`\`mermaid
erDiagram
    TABLE1 ||--o{ TABLE2 : "relationship"
    TABLE2 }o--|| TABLE3 : "relationship"
\`\`\`

**Relationship Details**:
- `table1.id` ‚Üí `table2.table1_id` (one-to-many)
- `table2.table3_id` ‚Üí `table3.id` (many-to-one)
```

## Instructions for LLM

1. Define all tables needed for Must-Have features
2. Include primary keys, foreign keys, and essential indexes
3. Use standard SQL types (UUID, VARCHAR, INTEGER, TIMESTAMP, etc.)
4. Show relationships in both diagram and text

## Example Output

```markdown
## Data Models

### Table: users
| Column | Type | Constraints |
|--------|------|-------------|
| id | UUID | PRIMARY KEY |
| email | VARCHAR(255) | UNIQUE, NOT NULL |
| password_hash | VARCHAR(255) | NOT NULL |
| created_at | TIMESTAMP | DEFAULT NOW() |
| updated_at | TIMESTAMP | DEFAULT NOW() |

**Indexes**:
- `idx_users_email` on `email`

---

### Table: tasks
| Column | Type | Constraints |
|--------|------|-------------|
| id | UUID | PRIMARY KEY |
| user_id | UUID | FOREIGN KEY (users.id), NOT NULL |
| title | VARCHAR(255) | NOT NULL |
| description | TEXT | |
| status | VARCHAR(50) | CHECK (status IN ('todo', 'in_progress', 'done')) |
| position | INTEGER | NOT NULL |
| created_at | TIMESTAMP | DEFAULT NOW() |
| updated_at | TIMESTAMP | DEFAULT NOW() |

**Indexes**:
- `idx_tasks_user_id` on `user_id`
- `idx_tasks_status` on `status`

## Relationships

\`\`\`mermaid
erDiagram
    users ||--o{ tasks : "owns"
\`\`\`

**Relationship Details**:
- `users.id` ‚Üí `tasks.user_id` (one-to-many): A user can have many tasks
```

## Ready?

Paste the feature catalog and user stories below, and I'll generate the data models.

```

## File: prompts/modules/requirements/feature_catalog.md
```md
# Feature Catalog Module

## Purpose
Generate a prioritized list of features using MoSCoW method.

## Input Required
- Product overview (from previous module)
- OR brief product description

## Output Format

```markdown
## Feature Catalog (MoSCoW)

### Must Have (MVP Core)
1. **[Feature Name]**: [1-sentence description]
2. **[Feature Name]**: [1-sentence description]
...

### Should Have (Post-MVP)
1. **[Feature Name]**: [1-sentence description]
...

### Could Have (Future)
1. **[Feature Name]**: [1-sentence description]
...

### Won't Have (Out of Scope)
1. **[Feature Name]**: [1-sentence reason why not]
...
```

## Instructions for LLM

1. **Must Have**: 5-10 features that are absolutely required for MVP
2. **Should Have**: 5-8 features that would be nice for v1.1
3. **Could Have**: 3-5 features for future consideration
4. **Won't Have**: 2-3 features explicitly out of scope

Keep each description to ONE sentence.

## Example Output

```markdown
## Feature Catalog (MoSCoW)

### Must Have (MVP Core)
1. **Task Board**: Kanban-style board with To Do, In Progress, Done columns
2. **Create Tasks**: Add tasks with title and markdown description
3. **Drag & Drop**: Move tasks between columns with drag-and-drop
4. **Local Storage**: Save tasks in browser (no backend required)
5. **GitHub Import**: One-click import of GitHub issues as tasks

### Should Have (Post-MVP)
1. **Tags & Filters**: Organize and filter tasks by custom tags
2. **Due Dates**: Set and track task deadlines
3. **Cloud Sync**: Save tasks to backend for multi-device access
4. **Keyboard Shortcuts**: Power-user shortcuts for common actions

### Could Have (Future)
1. **Time Tracking**: Track time spent on each task
2. **Subtasks**: Break down tasks into smaller subtasks
3. **Team Collaboration**: Share boards with team members

### Won't Have (Out of Scope)
1. **Gantt Charts**: Too complex for the target audience
2. **Resource Management**: Not needed for solo developers
3. **Billing/Invoicing**: Out of scope for a task management tool
```

## Ready?

Paste the product overview (or idea) below, and I'll generate the feature catalog.

```

## File: prompts/modules/requirements/product_overview.md
```md
# Product Overview Module

## Purpose
Generate a concise executive summary of the product idea.

## Input Required
- Brief description of the product idea
- Target users (if known)
- Problem being solved

## Output Format

```markdown
## Product Overview

**Name**: [Product Name]
**Tagline**: [One-sentence value proposition]

**The Problem**: [2-3 sentences describing the pain point]

**The Solution**: [2-3 sentences describing how the product solves it]

**Target Audience**:
- **Primary**: [Main user persona]
- **Secondary**: [Additional user personas]
```

## Instructions for LLM

1. Keep the product name short and memorable
2. Tagline should be under 10 words
3. Problem and Solution should each be 2-3 sentences max
4. Identify 1 primary and 1-2 secondary personas

## Example Output

```markdown
## Product Overview

**Name**: TaskFlow
**Tagline**: Simple task management for solo developers

**The Problem**: Developers need a lightweight way to track tasks without the complexity of enterprise project management tools. Existing solutions like Jira are overkill for solo projects, while simple to-do apps lack developer-specific features like GitHub integration and markdown support.

**The Solution**: TaskFlow provides a minimal Kanban board with drag-and-drop, markdown task descriptions, and one-click GitHub issue import. It's designed for developers who want just enough structure without the overhead.

**Target Audience**:
- **Primary**: Solo developers and freelancers managing personal projects
- **Secondary**: Small development teams (2-5 people) working on side projects
```

## Ready?

Paste the product idea below, and I'll generate the product overview.

```

## File: prompts/modules/requirements/user_stories.md
```md
# User Stories Module

## Purpose
Generate user stories with acceptance criteria for Must-Have features.

## Input Required
- Feature catalog (from previous module)
- OR list of Must-Have features

## Output Format

```markdown
## User Stories

### US-001: [Story Title]
**As a** [user type]
**I want** [action]
**So that** [benefit]

**Acceptance Criteria**:
- [ ] [Specific, testable criterion]
- [ ] [Specific, testable criterion]
- [ ] [Specific, testable criterion]

---

### US-002: [Story Title]
...
```

## Instructions for LLM

1. Create **1 user story per Must-Have feature**
2. Each story should have **3-5 acceptance criteria**
3. Acceptance criteria must be **specific and testable**
4. Use checkboxes for acceptance criteria

## Example Output

```markdown
## User Stories

### US-001: Create a Task
**As a** developer
**I want** to create a task with a title and markdown description
**So that** I can track what I need to do

**Acceptance Criteria**:
- [ ] User can click "New Task" button
- [ ] User can enter a task title (required field)
- [ ] User can enter a markdown description (optional field)
- [ ] Task appears in "To Do" column after creation
- [ ] Task has a unique ID and timestamp

---

### US-002: Move Tasks Between Columns
**As a** developer
**I want** to drag tasks between columns
**So that** I can update task status visually

**Acceptance Criteria**:
- [ ] User can click and drag a task card
- [ ] Task can be dropped into any column
- [ ] Task position updates immediately (no page refresh)
- [ ] Changes persist in local storage
- [ ] Dragging is smooth and responsive (< 100ms)

---

### US-003: Import GitHub Issues
**As a** developer
**I want** to import issues from my GitHub repo
**So that** I don't have to manually recreate tasks

**Acceptance Criteria**:
- [ ] User can enter a GitHub repo URL
- [ ] System validates the URL format
- [ ] System fetches open issues via GitHub API
- [ ] Issues appear as tasks in "To Do" column
- [ ] Issue title and description are preserved
```

## Ready?

Paste the feature catalog (or Must-Have features) below, and I'll generate the user stories.

```

## File: prompts/modules/security/threat_modeling.md
```md
# Threat Modeling Module

## Purpose
Generate a STRIDE threat model for the application.

## Input Required
- C4 diagrams
- Data models
- Feature catalog

## Output Format

```markdown
## Threat Model (STRIDE)

### Spoofing
| Threat | Attack Vector | Mitigation |
|--------|---------------|------------|
| [Threat description] | [How attacker could exploit] | [How we prevent it] |

### Tampering
| Threat | Attack Vector | Mitigation |
|--------|---------------|------------|
...

### Repudiation
...

### Information Disclosure
...

### Denial of Service
...

### Elevation of Privilege
...

## High-Priority Threats

1. **[Threat Name]** (Severity: High)
   - **Risk**: [Description]
   - **Mitigation**: [Action plan]
```

## Instructions for LLM

1. Identify 2-3 threats per STRIDE category
2. Focus on realistic, high-impact threats
3. Provide specific mitigations (not generic advice)
4. Highlight top 3-5 high-priority threats

## Example Output

```markdown
## Threat Model (STRIDE)

### Spoofing
| Threat | Attack Vector | Mitigation |
|--------|---------------|------------|
| User impersonation | Attacker steals session token | Use HTTP-only cookies, short session timeouts (15 min) |
| GitHub OAuth bypass | Attacker forges OAuth callback | Validate state parameter, verify token with GitHub |

### Tampering
| Threat | Attack Vector | Mitigation |
|--------|---------------|------------|
| Task data modification | Attacker modifies task via API | Validate user owns task before update, use CSRF tokens |
| SQL injection | Attacker injects SQL via task title | Use parameterized queries, input validation |

### Repudiation
| Threat | Attack Vector | Mitigation |
|--------|---------------|------------|
| Deny task deletion | User claims they didn't delete task | Log all delete operations with user ID and timestamp |

### Information Disclosure
| Threat | Attack Vector | Mitigation |
|--------|---------------|------------|
| Expose other users' tasks | Attacker guesses task IDs | Use UUIDs, enforce user ownership checks |
| Leak GitHub tokens | Token exposed in logs or errors | Never log tokens, use environment variables |

### Denial of Service
| Threat | Attack Vector | Mitigation |
|--------|---------------|------------|
| API rate limiting bypass | Attacker floods API with requests | Implement rate limiting (100 req/min per IP) |
| Database overload | Attacker creates millions of tasks | Limit tasks per user (1000 max) |

### Elevation of Privilege
| Threat | Attack Vector | Mitigation |
|--------|---------------|------------|
| Access admin endpoints | Attacker guesses admin routes | No admin routes in MVP, use role-based access later |

## High-Priority Threats

1. **SQL Injection** (Severity: High)
   - **Risk**: Attacker could read/modify all database data
   - **Mitigation**: Use ORM (Prisma) with parameterized queries, validate all inputs

2. **Session Hijacking** (Severity: High)
   - **Risk**: Attacker steals session token and impersonates user
   - **Mitigation**: HTTP-only cookies, HTTPS only, 15-minute session timeout

3. **Unauthorized Task Access** (Severity: Medium)
   - **Risk**: User A could view/modify User B's tasks
   - **Mitigation**: Enforce user ownership checks on all task operations
```

## Ready?

Paste the C4 diagrams, data models, and feature catalog below, and I'll generate the threat model.

```

## File: prompts/modules/testing/test_pyramid.md
```md
# Test Pyramid Module

## Purpose
Generate a test strategy following the test pyramid pattern.

## Input Required
- Feature catalog
- Data models
- API endpoints (if available)

## Output Format

```markdown
## Test Strategy (Test Pyramid)

### Unit Tests (70% of tests)
**What to Test**:
- Business logic functions
- Utility functions
- Data validation

**Example Test Cases**:
- `validateEmail()`: Valid email ‚Üí true, Invalid email ‚Üí false
- `calculateTaskPosition()`: Returns correct position based on column

**Coverage Goal**: 80% of business logic

---

### Integration Tests (20% of tests)
**What to Test**:
- API endpoints
- Database operations
- External service integrations

**Example Test Cases**:
- `POST /api/tasks`: Creates task and returns 201
- `GET /api/tasks`: Returns user's tasks only
- GitHub API integration: Fetches issues successfully

**Coverage Goal**: All API endpoints

---

### E2E Tests (10% of tests)
**What to Test**:
- Critical user flows
- Multi-step interactions

**Example Test Cases**:
- User signup ‚Üí login ‚Üí create task ‚Üí drag task ‚Üí logout
- Import GitHub issues ‚Üí verify tasks appear ‚Üí delete tasks

**Coverage Goal**: Top 3-5 user flows
```

## Instructions for LLM

1. Follow the 70/20/10 ratio (Unit/Integration/E2E)
2. Provide 3-5 example test cases per level
3. Focus on Must-Have features
4. Keep coverage goals realistic

## Example Output

```markdown
## Test Strategy (Test Pyramid)

### Unit Tests (70% of tests)
**What to Test**:
- Task validation logic
- Position calculation
- Status transitions
- Input sanitization

**Example Test Cases**:
- `validateTaskTitle()`: 
  - Valid title (1-255 chars) ‚Üí true
  - Empty title ‚Üí false
  - Title > 255 chars ‚Üí false
- `calculateTaskPosition()`:
  - First task in column ‚Üí position = 0
  - Task between two others ‚Üí position = average
- `sanitizeMarkdown()`:
  - Removes script tags
  - Preserves safe markdown

**Coverage Goal**: 80% of business logic

---

### Integration Tests (20% of tests)
**What to Test**:
- Task CRUD API endpoints
- User authentication
- GitHub API integration
- Database transactions

**Example Test Cases**:
- `POST /api/tasks`:
  - Valid request ‚Üí 201 Created
  - Missing title ‚Üí 400 Bad Request
  - Unauthorized ‚Üí 401 Unauthorized
- `GET /api/tasks`:
  - Returns only current user's tasks
  - Filters by status if provided
  - Returns empty array if no tasks
- `POST /api/github/import`:
  - Valid repo ‚Üí imports issues
  - Invalid repo ‚Üí 404 Not Found
  - Rate limit exceeded ‚Üí 429 Too Many Requests

**Coverage Goal**: All API endpoints

---

### E2E Tests (10% of tests)
**What to Test**:
- Complete user journeys
- Cross-browser compatibility
- Mobile responsiveness

**Example Test Cases**:
- **Happy Path**: 
  - User signs up ‚Üí verifies email ‚Üí logs in ‚Üí creates task ‚Üí drags to "Done" ‚Üí logs out
- **GitHub Import Flow**:
  - User connects GitHub ‚Üí selects repo ‚Üí imports issues ‚Üí verifies tasks created
- **Error Handling**:
  - User loses connection mid-drag ‚Üí task reverts ‚Üí error message shown

**Coverage Goal**: Top 5 user flows

## Test Tools

- **Unit**: Jest, Vitest
- **Integration**: Supertest, Playwright
- **E2E**: Playwright, Cypress
```

## Ready?

Paste the feature catalog and data models below, and I'll generate the test strategy.

```

## File: prompts/ultra-lean/00_overview.md
```md
# Product Overview (Ultra-Lean Mode)

## Your Role
You are helping define a **rapid prototype**. Keep it minimal‚Äîthis is for weekend projects and quick experiments.

## Input
The user will provide a brief product idea.

## Your Task

Generate a **Product Overview** with this exact structure:

```markdown
# Product Overview

**Name**: [Product Name]  
**Tagline**: [One-line description, max 10 words]

**Problem**: [2-3 sentences describing the pain point]

**Solution**: [2-3 sentences describing how the product solves it]

**Target Users**: [1-2 user types]
```

## Example Output

```markdown
# Product Overview

**Name**: QuickPoll  
**Tagline**: Dead-simple polling for teams

**Problem**: Teams waste time in email threads for simple yes/no decisions. Existing polling tools are too complex or lack persistence. Developers need something they can set up in 5 minutes.

**Solution**: QuickPoll lets you create a poll with a question and options, share a link, and see real-time results. No signup required for voters, results export to CSV.

**Target Users**: Small development teams (5-20 people), project managers
```

## Important Notes
- **Be concise**: This is Ultra-Lean mode‚Äîno walls of text
- **Focus on essentials**: Just the core problem and solution
- **No fluff**: Skip market analysis, competitive landscape, etc.

## Ready?

Paste your product idea below, and I'll generate the overview.

```

## File: prompts/ultra-lean/01_features.md
```md
# Core Features (Ultra-Lean Mode)

## Your Role
You are helping define the **absolute minimum** features for a rapid prototype.

## Input
Attach the product overview from the previous step.

## Your Task

Generate a **Core Features List** with this exact structure:

```markdown
# Core Features

## Must-Have (MVP)
1. **[Feature Name]**: [One-sentence description]
   - Acceptance: [1-2 criteria]

2. **[Feature Name]**: [One-sentence description]
   - Acceptance: [1-2 criteria]

[3-5 features total]
```

## Rules
- **3-5 features max** - If you can't build it in a weekend, it's not Ultra-Lean
- **One sentence per feature** - No detailed descriptions
- **1-2 acceptance criteria** - Just the bare minimum to know it works
- **Skip everything else** - No Should-Have, Could-Have, Won't-Have

## Example Output

```markdown
# Core Features

## Must-Have (MVP)
1. **Create Poll**: User can create a poll with a question and 2-5 options
   - Acceptance: Poll has unique URL, options are saved

2. **Vote**: Anyone with the link can vote once
   - Acceptance: Vote is recorded, user can't vote twice

3. **View Results**: See real-time vote counts
   - Acceptance: Results update without refresh, show percentages

4. **Share Link**: Copy poll URL to clipboard
   - Acceptance: URL works in any browser, no login required

5. **Export Results**: Download results as CSV
   - Acceptance: CSV includes question, options, vote counts
```

## Important Notes
- **Ruthlessly prioritize**: Only features you absolutely need for v0.1
- **No nice-to-haves**: Tags, filters, due dates, etc. can wait
- **Keep it simple**: If a feature needs 3+ acceptance criteria, it's too complex

## Ready?

Paste the product overview below, and I'll generate the core features.

```

## File: prompts/ultra-lean/02_system.md
```md
# System Diagram (Ultra-Lean Mode)

## Your Role
You are creating a **minimal system context diagram** for a rapid prototype.

## Input
Attach the product overview and core features.

## Your Task

Generate a **System Diagram** with this exact structure:

```markdown
# System Diagram

\`\`\`mermaid
C4Context
    title System Context - [Product Name]
    
    Person(user, "[User Type]", "Primary user")
    System(app, "[Product Name]", "Core application")
    System_Ext(external, "[External Service]", "Description")
    
    Rel(user, app, "Uses")
    Rel(app, external, "Integrates with")
\`\`\`

**Key Components**:
- **[Product Name]**: [One-sentence description]
- **[External Service]**: [One-sentence description]

**Tech Stack** (suggested):
- Frontend: [Framework]
- Backend: [Framework]
- Database: [Database]
- Hosting: [Platform]
```

## Rules
- **C4 Level 1 only** - Just system context, no containers or components
- **Show only essentials** - Main system + 1-2 external dependencies
- **One sentence per component** - No detailed descriptions
- **Suggest simple tech** - Proven, easy-to-deploy stack

## Example Output

```markdown
# System Diagram

\`\`\`mermaid
C4Context
    title System Context - QuickPoll
    
    Person(user, "Team Member", "Creates and votes on polls")
    System(quickpoll, "QuickPoll", "Polling application")
    System_Ext(browser, "Web Browser", "Displays UI")
    
    Rel(user, quickpoll, "Creates polls, votes")
    Rel(quickpoll, browser, "Renders UI")
\`\`\`

**Key Components**:
- **QuickPoll**: Web app that manages polls and votes
- **Web Browser**: Displays poll UI and results

**Tech Stack** (suggested):
- Frontend: React (Vite)
- Backend: Node.js (Express)
- Database: SQLite (local file)
- Hosting: Vercel (frontend) + Railway (backend)
```

## Important Notes
- **Keep it simple**: No microservices, no Kubernetes
- **Boring tech wins**: Use proven, well-documented tools
- **Optimize for speed**: Choose tech you can deploy in minutes

## Ready?

Paste the product overview and features below, and I'll generate the system diagram.

```

## File: prompts/ultra-lean/03_api.md
```md
# API Sketch (Ultra-Lean Mode)

## Your Role
You are creating a **minimal API list** for a rapid prototype.

## Input
Attach the core features and system diagram.

## Your Task

Generate an **API Sketch** with this exact structure:

```markdown
# API Sketch

## Endpoints

### [Resource Name]
- `GET /api/[resource]` - [One-sentence description]
- `POST /api/[resource]` - [One-sentence description]
- `GET /api/[resource]/:id` - [One-sentence description]
- `DELETE /api/[resource]/:id` - [One-sentence description]

[5-10 endpoints total]

## Data Models (Inferred)

### [Model Name]
- id, [field1], [field2], [field3]

[2-4 models total]
```

## Rules
- **5-10 endpoints max** - Just CRUD for core resources
- **RESTful conventions** - Use standard HTTP methods
- **One sentence per endpoint** - No detailed request/response schemas
- **Infer data models** - Just list key fields, no types/constraints

## Example Output

```markdown
# API Sketch

## Endpoints

### Polls
- `GET /api/polls` - List all polls
- `POST /api/polls` - Create a new poll
- `GET /api/polls/:id` - Get poll details and results
- `DELETE /api/polls/:id` - Delete a poll

### Votes
- `POST /api/polls/:id/vote` - Submit a vote
- `GET /api/polls/:id/results` - Get vote counts

### Export
- `GET /api/polls/:id/export` - Download results as CSV

## Data Models (Inferred)

### Poll
- id, question, options (array), created_at

### Vote
- id, poll_id, option_index, voter_ip, created_at
```

## Important Notes
- **Skip detailed schemas**: No OpenAPI, no request/response examples
- **Infer from features**: If feature needs data, add an endpoint
- **Keep it RESTful**: Use standard patterns (GET, POST, DELETE)

## Ready?

Paste the features and system diagram below, and I'll generate the API sketch.

```

## File: prompts/ultra-lean/04_plan.md
```md
# Build Plan (Ultra-Lean Mode)

## Your Role
You are creating a **minimal implementation plan** for a rapid prototype.

## Input
Attach all previous artifacts (overview, features, system, API).

## Your Task

Generate a **Build Plan** with this exact structure:

```markdown
# Build Plan

## Tasks

### 1. [Task Name]
**What**: [One-sentence description]  
**Deliverables**: [1-3 bullet points]  
**Time**: [Estimate in hours]

[5-10 tasks total]

## Timeline
- **Day 1**: Tasks 1-3
- **Day 2**: Tasks 4-6
- **Day 3**: Tasks 7-10

**Total**: [X] hours over [Y] days
```

## Rules
- **5-10 tasks max** - Each task should be 2-6 hours
- **Order by dependency** - Foundation first, polish last
- **Be realistic** - Don't underestimate time
- **Focus on MVP** - Only Must-Have features

## Example Output

```markdown
# Build Plan

## Tasks

### 1. Project Setup
**What**: Initialize project with chosen tech stack  
**Deliverables**:
- Vite + React app created
- Express server configured
- SQLite database initialized

**Time**: 2 hours

### 2. Database Schema
**What**: Create tables for polls and votes  
**Deliverables**:
- polls table (id, question, options, created_at)
- votes table (id, poll_id, option_index, voter_ip, created_at)
- Test data seeded

**Time**: 2 hours

### 3. Poll CRUD API
**What**: Implement poll creation and retrieval  
**Deliverables**:
- POST /api/polls endpoint
- GET /api/polls/:id endpoint
- DELETE /api/polls/:id endpoint

**Time**: 4 hours

### 4. Voting API
**What**: Implement vote submission and results  
**Deliverables**:
- POST /api/polls/:id/vote endpoint
- GET /api/polls/:id/results endpoint
- Prevent duplicate votes (by IP)

**Time**: 3 hours

### 5. Poll UI
**What**: Build poll creation and display interface  
**Deliverables**:
- Create poll form
- Display poll with options
- Show results with percentages

**Time**: 4 hours

### 6. Share & Export
**What**: Add link sharing and CSV export  
**Deliverables**:
- Copy link button
- CSV export endpoint
- Download results button

**Time**: 2 hours

### 7. Basic Styling
**What**: Make it look decent  
**Deliverables**:
- Responsive layout
- Clean typography
- Color scheme

**Time**: 2 hours

### 8. Deploy
**What**: Deploy to production  
**Deliverables**:
- Frontend on Vercel
- Backend on Railway
- Database migrated

**Time**: 2 hours

## Timeline
- **Day 1**: Tasks 1-3 (8 hours)
- **Day 2**: Tasks 4-6 (9 hours)
- **Day 3**: Tasks 7-8 (4 hours)

**Total**: 21 hours over 3 days
```

## Important Notes
- **Be honest with time**: Add 50% buffer to your estimates
- **Order matters**: Can't build UI before API
- **Keep it simple**: If a task is >6 hours, split it

## Ready?

Paste all previous artifacts below, and I'll generate the build plan.

```

